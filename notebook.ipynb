{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting readability-lxml\n",
      "  Downloading readability_lxml-0.8.1-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting chardet (from readability-lxml)\n",
      "  Using cached chardet-5.2.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting lxml (from readability-lxml)\n",
      "  Using cached lxml-5.3.1-cp310-cp310-win_amd64.whl.metadata (3.8 kB)\n",
      "Collecting cssselect (from readability-lxml)\n",
      "  Downloading cssselect-1.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Downloading readability_lxml-0.8.1-py3-none-any.whl (20 kB)\n",
      "Using cached chardet-5.2.0-py3-none-any.whl (199 kB)\n",
      "Downloading cssselect-1.3.0-py3-none-any.whl (18 kB)\n",
      "Using cached lxml-5.3.1-cp310-cp310-win_amd64.whl (3.8 MB)\n",
      "Installing collected packages: lxml, cssselect, chardet, readability-lxml\n",
      "Successfully installed chardet-5.2.0 cssselect-1.3.0 lxml-5.3.1 readability-lxml-0.8.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install readability-lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: newspaper3k in f:\\news_summarixation\\venv\\lib\\site-packages (0.2.8)\n",
      "Requirement already satisfied: beautifulsoup4>=4.4.1 in f:\\news_summarixation\\venv\\lib\\site-packages (from newspaper3k) (4.13.3)\n",
      "Requirement already satisfied: Pillow>=3.3.0 in f:\\news_summarixation\\venv\\lib\\site-packages (from newspaper3k) (11.1.0)\n",
      "Requirement already satisfied: PyYAML>=3.11 in f:\\news_summarixation\\venv\\lib\\site-packages (from newspaper3k) (6.0.2)\n",
      "Requirement already satisfied: cssselect>=0.9.2 in f:\\news_summarixation\\venv\\lib\\site-packages (from newspaper3k) (1.3.0)\n",
      "Requirement already satisfied: lxml>=3.6.0 in f:\\news_summarixation\\venv\\lib\\site-packages (from newspaper3k) (5.3.1)\n",
      "Requirement already satisfied: nltk>=3.2.1 in f:\\news_summarixation\\venv\\lib\\site-packages (from newspaper3k) (3.9.1)\n",
      "Requirement already satisfied: requests>=2.10.0 in f:\\news_summarixation\\venv\\lib\\site-packages (from newspaper3k) (2.32.3)\n",
      "Requirement already satisfied: feedparser>=5.2.1 in f:\\news_summarixation\\venv\\lib\\site-packages (from newspaper3k) (6.0.11)\n",
      "Requirement already satisfied: tldextract>=2.0.1 in f:\\news_summarixation\\venv\\lib\\site-packages (from newspaper3k) (5.1.3)\n",
      "Requirement already satisfied: feedfinder2>=0.0.4 in f:\\news_summarixation\\venv\\lib\\site-packages (from newspaper3k) (0.0.4)\n",
      "Requirement already satisfied: jieba3k>=0.35.1 in f:\\news_summarixation\\venv\\lib\\site-packages (from newspaper3k) (0.35.1)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in f:\\news_summarixation\\venv\\lib\\site-packages (from newspaper3k) (2.9.0.post0)\n",
      "Requirement already satisfied: tinysegmenter==0.3 in f:\\news_summarixation\\venv\\lib\\site-packages (from newspaper3k) (0.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in f:\\news_summarixation\\venv\\lib\\site-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.6)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in f:\\news_summarixation\\venv\\lib\\site-packages (from beautifulsoup4>=4.4.1->newspaper3k) (4.12.2)\n",
      "Requirement already satisfied: six in f:\\news_summarixation\\venv\\lib\\site-packages (from feedfinder2>=0.0.4->newspaper3k) (1.17.0)\n",
      "Requirement already satisfied: sgmllib3k in f:\\news_summarixation\\venv\\lib\\site-packages (from feedparser>=5.2.1->newspaper3k) (1.0.0)\n",
      "Requirement already satisfied: click in f:\\news_summarixation\\venv\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (8.1.8)\n",
      "Requirement already satisfied: joblib in f:\\news_summarixation\\venv\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in f:\\news_summarixation\\venv\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in f:\\news_summarixation\\venv\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (4.67.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in f:\\news_summarixation\\venv\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in f:\\news_summarixation\\venv\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in f:\\news_summarixation\\venv\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in f:\\news_summarixation\\venv\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (2025.1.31)\n",
      "Requirement already satisfied: requests-file>=1.4 in f:\\news_summarixation\\venv\\lib\\site-packages (from tldextract>=2.0.1->newspaper3k) (2.1.0)\n",
      "Requirement already satisfied: filelock>=3.0.8 in f:\\news_summarixation\\venv\\lib\\site-packages (from tldextract>=2.0.1->newspaper3k) (3.18.0)\n",
      "Requirement already satisfied: colorama in f:\\news_summarixation\\venv\\lib\\site-packages (from click->nltk>=3.2.1->newspaper3k) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install newspaper3k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lxml[html_clean] in f:\\news_summarixation\\venv\\lib\\site-packages (5.3.1)\n",
      "Requirement already satisfied: lxml_html_clean in f:\\news_summarixation\\venv\\lib\\site-packages (from lxml[html_clean]) (0.4.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install lxml[html_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-groq in f:\\news_summarixation\\venv\\lib\\site-packages (0.2.5)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.42 in f:\\news_summarixation\\venv\\lib\\site-packages (from langchain-groq) (0.3.45)\n",
      "Requirement already satisfied: groq<1,>=0.4.1 in f:\\news_summarixation\\venv\\lib\\site-packages (from langchain-groq) (0.19.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in f:\\news_summarixation\\venv\\lib\\site-packages (from groq<1,>=0.4.1->langchain-groq) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in f:\\news_summarixation\\venv\\lib\\site-packages (from groq<1,>=0.4.1->langchain-groq) (1.9.0)\n",
      "Collecting httpx<1,>=0.23.0 (from groq<1,>=0.4.1->langchain-groq)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in f:\\news_summarixation\\venv\\lib\\site-packages (from groq<1,>=0.4.1->langchain-groq) (2.10.6)\n",
      "Requirement already satisfied: sniffio in f:\\news_summarixation\\venv\\lib\\site-packages (from groq<1,>=0.4.1->langchain-groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in f:\\news_summarixation\\venv\\lib\\site-packages (from groq<1,>=0.4.1->langchain-groq) (4.12.2)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in f:\\news_summarixation\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.42->langchain-groq) (0.3.15)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in f:\\news_summarixation\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.42->langchain-groq) (9.0.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in f:\\news_summarixation\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.42->langchain-groq) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in f:\\news_summarixation\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.42->langchain-groq) (6.0.2)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in f:\\news_summarixation\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.42->langchain-groq) (24.2)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in f:\\news_summarixation\\venv\\lib\\site-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain-groq) (1.2.2)\n",
      "Requirement already satisfied: idna>=2.8 in f:\\news_summarixation\\venv\\lib\\site-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain-groq) (2.10)\n",
      "Requirement already satisfied: certifi in f:\\news_summarixation\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (2025.1.31)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq)\n",
      "  Using cached httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in f:\\news_summarixation\\venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.42->langchain-groq) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in f:\\news_summarixation\\venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.42->langchain-groq) (3.10.15)\n",
      "Requirement already satisfied: requests<3,>=2 in f:\\news_summarixation\\venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.42->langchain-groq) (2.32.3)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in f:\\news_summarixation\\venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.42->langchain-groq) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in f:\\news_summarixation\\venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.42->langchain-groq) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in f:\\news_summarixation\\venv\\lib\\site-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain-groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in f:\\news_summarixation\\venv\\lib\\site-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain-groq) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in f:\\news_summarixation\\venv\\lib\\site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.42->langchain-groq) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in f:\\news_summarixation\\venv\\lib\\site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.42->langchain-groq) (2.3.0)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Installing collected packages: h11, httpcore, httpx\n",
      "  Attempting uninstall: h11\n",
      "    Found existing installation: h11 0.9.0\n",
      "    Uninstalling h11-0.9.0:\n",
      "      Successfully uninstalled h11-0.9.0\n",
      "  Attempting uninstall: httpcore\n",
      "    Found existing installation: httpcore 0.9.1\n",
      "    Uninstalling httpcore-0.9.1:\n",
      "      Successfully uninstalled httpcore-0.9.1\n",
      "  Attempting uninstall: httpx\n",
      "    Found existing installation: httpx 0.13.3\n",
      "    Uninstalling httpx-0.13.3:\n",
      "      Successfully uninstalled httpx-0.13.3\n",
      "Successfully installed h11-0.14.0 httpcore-1.0.7 httpx-0.28.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "googletrans 4.0.0rc1 requires httpx==0.13.3, but you have httpx 0.28.1 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain-groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching article [1]: https://www.indianweb2.com/2025/03/tesla-launching-2-e-car-models-in-india.html\n",
      "\n",
      "Fetching article [2]: https://www.telegraphindia.com/world/as-tesla-tanks-elon-musks-chosen-board-chair-thrives/cid/2089317\n",
      "\n",
      "Fetching article [3]: https://www.timesnownews.com/auto/electric-vehicles/tesla-initiates-homologation-of-model-y-and-model-3-for-india-article-119108577\n",
      "\n",
      "Fetching article [4]: https://www.msn.com/en-in/news/other/internet-slams-kim-kardashian-s-tesla-photoshoot-as-anti-musk-protests-gain-momentum/ar-AA1B26lN?ocid=BingNewsVerp\n",
      "\n",
      "Fetching article [5]: https://www.ndtv.com/auto/tesla-model-3-model-ys-homologation-process-initiated-in-india-7940909\n",
      "\n",
      "Fetching article [6]: https://www.dailymail.co.uk/sciencetech/article-14506405/Elon-Musk-HALT-Cybertruck-deliveries.html\n",
      "\n",
      "Fetching article [7]: https://www.msn.com/en-ie/technology/tech-companies/more-cybertruck-problems-for-elon-musk-tesla-and-customers/ar-AA1B1a66?ocid=BingNewsVerp\n",
      "\n",
      "Fetching article [8]: https://www.msn.com/en-in/entertainment/bollywood/as-tesla-tanks-musk-s-chosen-board-chair-stands-strong-as-a-badass-woman-in-business-world/ar-AA1B4QO5?ocid=BingNewsVerp\n",
      "\n",
      "Fetching article [9]: https://www.reuters.com/investigations/tesla-tanks-musks-hand-picked-board-chair-is-doing-just-fine-2025-03-17/\n",
      "\n",
      "Fetching article [10]: https://www.msn.com/en-xl/news/other/tesla-planned-to-sell-the-cybertruck-for-100000-forever-now-it-s-already-selling-for-much-less/ar-AA1AYPpX?ocid=BingNewsVerp\n",
      "\n",
      "\n",
      "Final Output:\n",
      "\n",
      "{'Company': 'tesla', 'Articles': [{'Title': 'Tesla Launching 2 e-Car Models in India, Files Homologation Application', 'Summary': \"This is a great concise summary of the speech! You've effectively captured the key features and differentiating points of both the Model 3 and Model Y. \\n\\nHere are a few minor suggestions:\\n\\n* **Target Audience:**  It might be helpful to mention who the target audience for this speech is. Is it potential buyers, investors, or the general public? This context can further strengthen the summary.\\n* **Call to Action:**  Does the speech include a call to action? For example, does it encourage viewers to test drive a Tesla or visit a showroom?  Mentioning this could add another layer to the summary.\\n\\nOverall, your summary is clear, accurate, and well-organized. It effectively communicates the essence of the speech and highlights the unique selling points of both the Model 3 and Model Y. \\n\\n\\n\\n\", 'Sentiment': 'Positive', 'Topics': ['Model Y. \\n\\n', 'Model Y. \\n\\n\\n\\n']}, {'Title': 'As Tesla tanks, Elon Musk’s chosen board chair thrives - Telegraph India', 'Summary': \"These summaries paint a complex picture of Robyn Denholm, highlighting both her achievements and the controversies surrounding her. \\n\\nHere's a breakdown of the key themes:\\n\\n**Denholm's Success:**\\n\\n* **Career Trajectory:** She's portrayed as a highly successful executive who rose through the ranks at major companies like Toyota, Sun Microsystems, and Juniper Networks. \\n* **Financial Acumen:** Denholm is recognized for her financial expertise and ability to navigate complex business situations, especially during challenging times like the 2008 financial crisis.\\n* **Leadership at Tesla:** Despite initial skepticism, she's credited with helping Tesla achieve significant growth and financial success during her tenure as Chair.\\n\\n**Controversies and Criticisms:**\\n\\n* **High Compensation:** Denholm's substantial earnings from Tesla stock, especially in comparison to other board members, are a major point of contention. Critics argue it raises conflicts of interest and questions about her independence.\\n* **Musk's Influence:** Concerns are raised about Denholm's close relationship with Elon Musk and whether she effectively oversees him, particularly given his controversial actions and distractions.\\n* **Lack of Transparency:**\\n\\nQuestions are raised about Denholm's commitment to Tesla and her various other commitments outside the company, leading to calls for greater transparency about her time allocation.\\n\\n\\n**Overall:**\\n\\nThe summaries present a nuanced view of Robyn Denholm. While her professional achievements are undeniable, her close association with Elon Musk and the questions surrounding her compensation and oversight cast a shadow over her leadership. The articles suggest that Denholm faces a significant challenge in balancing her own financial interests with the needs of Tesla and its shareholders. \\n\\n\\nIt's important to note that these are just summaries, and a deeper understanding of the issues would require reading the full articles.\\n\", 'Sentiment': 'Positive', 'Topics': ['Chair', 'Elon Musk', 'Denholm', 'Criticisms:**', 'Juniper Networks', 'Toyota', 'Success:**', 'Robyn Denholm', 'Sun Microsystems']}, {'Title': 'Tesla Initiates Homologation Of These Two Electric Cars For India', 'Summary': \"This is an excellent summary of the Tesla news and the broader Indian EV market! \\n\\n**Here's a breakdown of its strengths:**\\n\\n* **Concise and clear:** You've efficiently conveyed the key points about Tesla's homologation process, its potential impact on the Indian market, and the existing competitive landscape.\\n* **Contextualization:**  You effectively connect Tesla's move to the larger context of the booming Indian EV market, highlighting government support and growth figures.\\n* **Specific details:** You provide valuable figures like the 20% growth in electric passenger vehicle sales and the 32% increase in electric two-wheeler sales. \\n* **Future outlook:** You mention the projected 43% CAGR for EV sales, painting a picture of continued growth.\\n\\n**Possible enhancements:**\\n\\n* **Sources:** Consider adding brief mentions of the sources for the EV sales data to enhance credibility. \\n* **Challenges:** While focusing on the positive growth, briefly mentioning potential challenges Tesla might face in India (e.g., infrastructure, local competition) could add depth. \\n\\n\\n\\nOverall, this is a well-written and informative summary. \\n\\n\", 'Sentiment': 'Positive', 'Topics': ['India', 'Tesla', 'EV']}, {'Title': 'MSN', 'Summary': \"Please provide me with the speech text! I'm ready to summarize it for you. 😊  \\n\", 'Sentiment': 'Positive', 'Topics': []}, {'Title': \"Tesla Model 3, Model Y's Homologation Process Initiated In India\", 'Summary': \"This is a very good concise summary of the Tesla news! \\n\\nIt covers all the key points:\\n\\n* **Homologation process initiated:** This is the major news and signifies Tesla's serious commitment to entering the Indian market.\\n* **Timeline and context:** You accurately place this development within the larger context of Tesla's showroom opening, hiring, and the ongoing trade negotiations between India and the US.\\n* **Import strategy:**  Clearly outlining the initial import plan as CBUs adds valuable detail about Tesla's approach.\\n* **Global manufacturing network:**  Mentioning Tesla's existing and upcoming manufacturing locations provides a broader perspective on their global strategy.\\n\\n\\nOverall, this summary is well-written, informative, and easy to understand.  \\n\\n\", 'Sentiment': 'Positive', 'Topics': ['US', 'India', 'Tesla']}, {'Title': 'Error extracting content', 'Summary': 'You are absolutely right!  My apologies, I seem to have misread the input. \\n\\nIt is indeed an error message, not a speech, and you\\'ve accurately explained the cause: a \"ConnectTimeoutError\" preventing the program from reaching the Daily Mail website. \\n\\nThanks for pointing out my mistake!  I am still under development and learning to interpret information correctly.  \\n\\n', 'Sentiment': 'Positive', 'Topics': ['the Daily Mail']}, {'Title': 'MSN', 'Summary': \"Please provide me with the speech text. I need the actual words of the speech to be able to summarize it.  \\n\\nI apologize for the previous error. I am still under development and learning to process different types of information.  \\n\\nOnce you give me the speech text, I'll do my best to provide a clear and concise summary.  \\n\\n\", 'Sentiment': 'Positive', 'Topics': []}, {'Title': 'MSN', 'Summary': \"Please provide me with the speech text! I'm ready to summarize it for you. 😊  \\n\\n\", 'Sentiment': 'Positive', 'Topics': []}, {'Title': 'Error extracting content', 'Summary': \"Please provide me with the speech text so I can summarize it for you. \\n\\nI understand you mentioned a link to a Reuters article, but I am unable to access external websites or specific files online.  \\n\\nOnce you provide the text, I'll be happy to help! \\n\\n\", 'Sentiment': 'Positive', 'Topics': ['Reuters']}, {'Title': 'MSN', 'Summary': \"Please provide me with the speech text! I'm ready to analyze it and create a concise summary for you.  😊  \\n\\n\", 'Sentiment': 'Positive', 'Topics': []}], 'Final Sentiment Analysis': 'The latest news coverage is mostly positive. Potential stock growth expected.'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from newspaper import Article\n",
    "import time\n",
    "import os\n",
    "import spacy\n",
    "from dotenv import load_dotenv\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.schema import Document\n",
    "from textblob import TextBlob  \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "# Load NLP model for topic extraction\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Initialize LLM\n",
    "llm = ChatGroq(groq_api_key=api_key, model=\"Gemma2-9b-It\")\n",
    "\n",
    "def fetch_news_links_bing(company, max_articles=5):\n",
    "    \"\"\"Fetch at least 5 unique news article links from Bing News\"\"\"\n",
    "    search_url = f\"https://www.bing.com/news/search?q={company.replace(' ', '+')}\"\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "    response = requests.get(search_url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    news_links = set()\n",
    "    for link in soup.find_all(\"a\", href=True):\n",
    "        url = link[\"href\"]\n",
    "        if url.startswith(\"http\") and \"bing.com\" not in url and url not in news_links:\n",
    "            news_links.add(url)\n",
    "        if len(news_links) >= max_articles:\n",
    "            break\n",
    "\n",
    "    if len(news_links) < max_articles:\n",
    "        print(f\"Warning: Only {len(news_links)} unique articles found.\")\n",
    "    \n",
    "    return list(news_links)[:max_articles]\n",
    "\n",
    "def extract_article_text(url):\n",
    "    \"\"\"Extract clean article text using newspaper3k\"\"\"\n",
    "    try:\n",
    "        article = Article(url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        \n",
    "        return article.title, article.text if article.text else \"Could not extract article content.\"\n",
    "    except Exception as e:\n",
    "        return \"Error extracting content\", f\"Error extracting content: {str(e)}\"\n",
    "\n",
    "def extract_key_topics(text):\n",
    "    \"\"\"Extracts key topics using spaCy NLP\"\"\"\n",
    "    doc = nlp(text)\n",
    "    topics = set()\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in [\"ORG\", \"GPE\", \"EVENT\", \"PERSON\"]:\n",
    "            topics.add(ent.text)\n",
    "    \n",
    "    return list(topics)\n",
    "\n",
    "def summarize_text(speech_text):\n",
    "    \"\"\"Summarizes the article content\"\"\"\n",
    "    \n",
    "    docs = [Document(page_content=speech_text)]\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=100)\n",
    "    final_documents = text_splitter.split_documents(docs)\n",
    "\n",
    "    chunks_prompt = PromptTemplate(\n",
    "        input_variables=['text'],\n",
    "        template=\"Please summarize the below speech:\\nSpeech: `{text}`\\nSummary:\"\n",
    "    )\n",
    "\n",
    "    final_prompt_template = PromptTemplate(\n",
    "        input_variables=['text'],\n",
    "        template=\"Here is a concise summary of the speech:\\n\\n{text}\"\n",
    "    )\n",
    "\n",
    "    summary_chain = load_summarize_chain(\n",
    "        llm=llm,\n",
    "        chain_type=\"map_reduce\",\n",
    "        map_prompt=chunks_prompt,\n",
    "        combine_prompt=final_prompt_template,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    summary_output = summary_chain.invoke({\"input_documents\": final_documents})['output_text']\n",
    "    return summary_output\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    \"\"\"Performs sentiment analysis using TextBlob\"\"\"\n",
    "    sentiment_score = TextBlob(text).sentiment.polarity\n",
    "    if sentiment_score > 0:\n",
    "        return \"Positive\"\n",
    "    elif sentiment_score < 0:\n",
    "        return \"Negative\"\n",
    "    else:\n",
    "        return \"Neutral\"\n",
    "\n",
    "def overall_sentiment_analysis(articles):\n",
    "    \"\"\"Determines overall sentiment from multiple articles.\"\"\"\n",
    "    sentiment_counts = {\"Positive\": 0, \"Negative\": 0, \"Neutral\": 0}\n",
    "\n",
    "    for article in articles:\n",
    "        sentiment_counts[article[\"Sentiment\"]] += 1\n",
    "\n",
    "    if sentiment_counts[\"Positive\"] > sentiment_counts[\"Negative\"]:\n",
    "        return \"The latest news coverage is mostly positive. Potential stock growth expected.\"\n",
    "    elif sentiment_counts[\"Negative\"] > sentiment_counts[\"Positive\"]:\n",
    "        return \"The latest news coverage is mostly negative. Stock decline possible.\"\n",
    "    else:\n",
    "        return \"The news coverage is balanced. Market stability expected.\"\n",
    "\n",
    "def compare_articles(articles):\n",
    "    texts = [article[\"Summary\"] for article in articles]\n",
    "    vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "    tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "    \n",
    "    comparison_results = []\n",
    "    for i in range(len(articles)):\n",
    "        for j in range(i + 1, len(articles)):\n",
    "            similarity = (tfidf_matrix[i].dot(tfidf_matrix[j].T)).toarray()[0, 0]\n",
    "            comparison_results.append({\n",
    "                \"article_1\": articles[i][\"Title\"],\n",
    "                \"article_2\": articles[j][\"Title\"],\n",
    "                \"similarity_score\": round(float(similarity), 2)\n",
    "            })\n",
    "    \n",
    "    return comparison_results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    company_name = input(\"Enter company name: \")\n",
    "    news_links = fetch_news_links_bing(company_name, max_articles=10)\n",
    "\n",
    "    articles_data = []\n",
    "\n",
    "    if news_links:\n",
    "        for idx, link in enumerate(news_links):\n",
    "            print(f\"\\nFetching article [{idx+1}]: {link}\")\n",
    "            time.sleep(1)\n",
    "            \n",
    "            title, article_text = extract_article_text(link)\n",
    "            summary = summarize_text(article_text)\n",
    "            key_topics = extract_key_topics(summary)\n",
    "            sentiment = analyze_sentiment(summary)\n",
    "            \n",
    "            articles_data.append({\n",
    "                \"Title\": title,\n",
    "                \"Summary\": summary,\n",
    "                \"Sentiment\": sentiment,\n",
    "                \"Topics\": key_topics\n",
    "            })\n",
    "\n",
    "    final_sentiment = overall_sentiment_analysis(articles_data)\n",
    "\n",
    "    output_data = {\n",
    "        \"Company\": company_name,\n",
    "        \"Articles\": articles_data,\n",
    "        \"Final Sentiment Analysis\": final_sentiment\n",
    "    }\n",
    "\n",
    "    print(\"\\n\\nFinal Output:\\n\")\n",
    "    print(output_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: groq in f:\\news_summarixation\\venv\\lib\\site-packages (0.19.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in f:\\news_summarixation\\venv\\lib\\site-packages (from groq) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in f:\\news_summarixation\\venv\\lib\\site-packages (from groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in f:\\news_summarixation\\venv\\lib\\site-packages (from groq) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in f:\\news_summarixation\\venv\\lib\\site-packages (from groq) (2.10.6)\n",
      "Requirement already satisfied: sniffio in f:\\news_summarixation\\venv\\lib\\site-packages (from groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in f:\\news_summarixation\\venv\\lib\\site-packages (from groq) (4.12.2)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in f:\\news_summarixation\\venv\\lib\\site-packages (from anyio<5,>=3.5.0->groq) (1.2.2)\n",
      "Requirement already satisfied: idna>=2.8 in f:\\news_summarixation\\venv\\lib\\site-packages (from anyio<5,>=3.5.0->groq) (2.10)\n",
      "Requirement already satisfied: certifi in f:\\news_summarixation\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in f:\\news_summarixation\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in f:\\news_summarixation\\venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in f:\\news_summarixation\\venv\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in f:\\news_summarixation\\venv\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (2.27.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching article [1]: https://www.msn.com/en-in/entertainment/bollywood/as-tesla-tanks-musk-s-chosen-board-chair-stands-strong-as-a-badass-woman-in-business-world/ar-AA1B4QO5?ocid=BingNewsVerp\n",
      "\n",
      "Fetching article [2]: https://www.dailymail.co.uk/sciencetech/article-14506405/Elon-Musk-HALT-Cybertruck-deliveries.html\n",
      "\n",
      "Fetching article [3]: https://www.msn.com/en-xl/news/other/tesla-planned-to-sell-the-cybertruck-for-100000-forever-now-it-s-already-selling-for-much-less/ar-AA1AYPpX?ocid=BingNewsVerp\n",
      "\n",
      "Fetching article [4]: https://www.indianweb2.com/2025/03/tesla-launching-2-e-car-models-in-india.html\n",
      "\n",
      "Fetching article [5]: https://www.msn.com/en-ie/technology/tech-companies/more-cybertruck-problems-for-elon-musk-tesla-and-customers/ar-AA1B1a66?ocid=BingNewsVerp\n",
      "\n",
      "Fetching article [6]: https://www.ndtv.com/auto/tesla-model-3-model-ys-homologation-process-initiated-in-india-7940909\n",
      "\n",
      "Fetching article [7]: https://www.timesnownews.com/auto/electric-vehicles/tesla-begins-certification-process-for-model-y-model-3-in-india-article-119120768\n",
      "\n",
      "Fetching article [8]: https://www.reuters.com/investigations/tesla-tanks-musks-hand-picked-board-chair-is-doing-just-fine-2025-03-17/\n",
      "\n",
      "Fetching article [9]: https://www.telegraphindia.com/world/as-tesla-tanks-elon-musks-chosen-board-chair-thrives/cid/2089317\n",
      "\n",
      "Fetching article [10]: https://www.msn.com/en-in/news/techandscience/musks-mars-mission-spacexs-starship-ready-to-blast-off-with-tesla-optimus-by/ar-AA1B0B1Y?ocid=BingNewsVerp\n",
      "\n",
      "\n",
      "Final Output:\n",
      "\n",
      "{'Company': 'tesla', 'Articles': [{'Title': 'MSN', 'Summary': \"Please paste the speech text here, and I'll be happy to summarize it for you! 😊  \\n\", 'Sentiment': 'Positive', 'Topics': []}, {'Title': 'Error extracting content', 'Summary': 'This is a great summary! \\n\\nIt clearly and concisely explains the problem:\\n\\n* **Technical issue:**  A connection error prevented content extraction.\\n* **Specific error:**  \"ConnectTimeoutError\" - the system couldn\\'t connect within 7 seconds.\\n* **Implication:**  The speech doesn\\'t offer any information about the Daily Mail article\\'s content. \\n\\n\\nYou\\'ve effectively captured the essence of the situation.  \\n\\n', 'Sentiment': 'Positive', 'Topics': ['Daily Mail']}, {'Title': 'MSN', 'Summary': \"Please provide me with the speech! I'm eager to read it and create a concise summary for you.  \\n\\n\", 'Sentiment': 'Positive', 'Topics': []}, {'Title': 'Tesla Launching 2 e-Car Models in India, Files Homologation Application', 'Summary': 'This is a great concise summary of the speech!  \\n\\nHere are a few minor suggestions to make it even stronger:\\n\\n* **Target Audience:** Consider adding a sentence about who the speech is aimed at (e.g., potential customers, investors, the general public). This helps establish context.\\n* **Emphasis on Innovation:**  You could highlight that Tesla is known for its cutting-edge technology and how these models exemplify that, especially with features like over-the-air updates and advanced driver assistance.\\n* **Call to Action:** If the speech is meant to drive sales, consider adding a sentence that encourages the audience to learn more or visit a Tesla showroom.\\n\\n\\nOverall, your summary effectively captures the key points of the speech and provides a clear understanding of the Tesla Model 3 and Model Y. \\n\\n', 'Sentiment': 'Positive', 'Topics': ['the Tesla Model 3', 'Model Y.', 'Tesla']}, {'Title': 'MSN', 'Summary': \"Please provide me with the speech text! 😊  I'm ready to analyze it and create a concise summary for you.  \\n\\n\", 'Sentiment': 'Positive', 'Topics': []}, {'Title': \"Tesla Model 3, Model Y's Homologation Process Initiated In India\", 'Summary': \"This is a great concise summary! It covers all the key points:\\n\\n* **Tesla starts homologation process:** This is the critical first step towards selling cars in India.\\n* **Showroom and hiring:** Shows Tesla is serious about entering the market.\\n* **Timing:**  The announcement coincides with trade talks, which could be beneficial for Tesla.\\n* **Import strategy:** Initial focus on importing, with potential future manufacturing.\\n* **Existing manufacturing locations:**  Provides context for Tesla's global production capacity.\\n\\n\\nYou've effectively captured the essence of the news in a clear and informative way.  \\n\\n\", 'Sentiment': 'Positive', 'Topics': ['India', 'Tesla']}, {'Title': 'Tesla Begins Certification Process for Model Y, Model 3 in India', 'Summary': \"This is a great summary of the Tesla news and the broader Indian EV market! \\n\\nHere are a few thoughts:\\n\\n* **Tesla's entry:** The news about Tesla's homologation applications is significant for the Indian market. It signals Tesla's serious intent to enter the country, even if manufacturing plans are unclear. The potential for a US-India free trade agreement could further incentivize Tesla's move.\\n* **Competition:**  It's interesting to note that Tesla might be bypassing China.  The Indian market, already robust with players like Tata Motors and JSW MG Motors, will undoubtedly become even more competitive with Tesla's arrival.\\n* **EV Market Growth:** The EV sales data paints a very positive picture for the Indian market. The high growth rate, driven by government initiatives and consumer demand, suggests a strong future for electric mobility in India.  \\n\\n**Potential areas for expansion:**\\n\\n* **Local Manufacturing:**  It would be interesting to see if Tesla eventually decides to manufacture locally in India. This could potentially lead to cost reductions and further stimulate the Indian EV ecosystem.\\n* **Tesla's Impact:**  How will Tesla's entry impact existing Indian EV manufacturers? Will it accelerate innovation and drive down prices? \\n* **Charging Infrastructure:** The growth of EVs will depend heavily on the development of a robust charging infrastructure.  \\n\\n\\n\\nOverall, this is a fascinating development that has the potential to reshape the Indian automotive landscape.  \\n\\n\", 'Sentiment': 'Positive', 'Topics': ['India', 'Tata Motors', 'US', 'Tesla', 'EV', 'China', 'JSW MG Motors']}, {'Title': 'Error extracting content', 'Summary': 'Please provide me with the text of the speech so I can summarize it for you. I need the actual words of the speech to understand its content and create a summary. \\n\\nI apologize for the inconvenience caused by the previous error. \\n\\n', 'Sentiment': 'Negative', 'Topics': []}, {'Title': 'As Tesla tanks, Elon Musk’s chosen board chair thrives - Telegraph India', 'Summary': \"This is a great collection of summaries! You've clearly done a good job of capturing the key points and themes of these articles about Robyn Denholm and her relationship with Tesla.  \\n\\nHere are some observations and suggestions:\\n\\n**Strengths:**\\n\\n* **Concise and informative:** Each summary is brief yet provides a good overview of the article's main points.\\n* **Focus on key themes:** You've consistently highlighted the central issues, such as Denholm's compensation, her independence as chair, her role in Tesla's success, and the potential conflicts of interest.\\n* **Neutral tone:** The summaries maintain a neutral and objective tone, avoiding personal opinions or judgments.\\n\\n**Suggestions:**\\n\\n* **Consider adding a brief introduction:** A short introductory sentence or two could provide context for the collection of summaries and highlight the overall focus.\\n* **Connect the summaries:** You could add brief transitions or linking phrases between summaries to create a more cohesive narrative.\\n* **Highlight contrasting perspectives:** Several summaries touch on differing viewpoints about Denholm's performance and compensation. Emphasizing these contrasting perspectives could add depth and complexity to the collection.\\n* **Consider audience:** Depending on the intended audience, you might want to adjust the level of detail or focus on specific aspects of Denholm's story.\\n\\n\\nOverall, you've done a commendable job of summarizing these articles. With a few minor tweaks, you can create a more engaging and insightful collection. \\n\\n\", 'Sentiment': 'Positive', 'Topics': ['Robyn Denholm', 'Denholm', 'Tesla']}, {'Title': 'MSN', 'Summary': \"Please share the speech content with me! I'm ready to read it and provide a concise summary.  \\n\\n\", 'Sentiment': 'Positive', 'Topics': []}], 'Final Sentiment Analysis': [{'article_1': 'MSN', 'article_2': 'Error extracting content', 'similarity_score': 0.02}, {'article_1': 'MSN', 'article_2': 'MSN', 'similarity_score': 0.06}, {'article_1': 'MSN', 'article_2': 'Tesla Launching 2 e-Car Models in India, Files Homologation Application', 'similarity_score': 0.06}, {'article_1': 'MSN', 'article_2': 'MSN', 'similarity_score': 0.19}, {'article_1': 'MSN', 'article_2': \"Tesla Model 3, Model Y's Homologation Process Initiated In India\", 'similarity_score': 0.0}, {'article_1': 'MSN', 'article_2': 'Tesla Begins Certification Process for Model Y, Model 3 in India', 'similarity_score': 0.0}, {'article_1': 'MSN', 'article_2': 'Error extracting content', 'similarity_score': 0.23}, {'article_1': 'MSN', 'article_2': 'As Tesla tanks, Elon Musk’s chosen board chair thrives - Telegraph India', 'similarity_score': 0.0}, {'article_1': 'MSN', 'article_2': 'MSN', 'similarity_score': 0.06}, {'article_1': 'Error extracting content', 'article_2': 'MSN', 'similarity_score': 0.04}, {'article_1': 'Error extracting content', 'article_2': 'Tesla Launching 2 e-Car Models in India, Files Homologation Application', 'similarity_score': 0.05}, {'article_1': 'Error extracting content', 'article_2': 'MSN', 'similarity_score': 0.04}, {'article_1': 'Error extracting content', 'article_2': \"Tesla Model 3, Model Y's Homologation Process Initiated In India\", 'similarity_score': 0.08}, {'article_1': 'Error extracting content', 'article_2': 'Tesla Begins Certification Process for Model Y, Model 3 in India', 'similarity_score': 0.01}, {'article_1': 'Error extracting content', 'article_2': 'Error extracting content', 'similarity_score': 0.17}, {'article_1': 'Error extracting content', 'article_2': 'As Tesla tanks, Elon Musk’s chosen board chair thrives - Telegraph India', 'similarity_score': 0.06}, {'article_1': 'Error extracting content', 'article_2': 'MSN', 'similarity_score': 0.14}, {'article_1': 'MSN', 'article_2': 'Tesla Launching 2 e-Car Models in India, Files Homologation Application', 'similarity_score': 0.11}, {'article_1': 'MSN', 'article_2': 'MSN', 'similarity_score': 0.43}, {'article_1': 'MSN', 'article_2': \"Tesla Model 3, Model Y's Homologation Process Initiated In India\", 'similarity_score': 0.04}, {'article_1': 'MSN', 'article_2': 'Tesla Begins Certification Process for Model Y, Model 3 in India', 'similarity_score': 0.01}, {'article_1': 'MSN', 'article_2': 'Error extracting content', 'similarity_score': 0.23}, {'article_1': 'MSN', 'article_2': 'As Tesla tanks, Elon Musk’s chosen board chair thrives - Telegraph India', 'similarity_score': 0.07}, {'article_1': 'MSN', 'article_2': 'MSN', 'similarity_score': 0.49}, {'article_1': 'Tesla Launching 2 e-Car Models in India, Files Homologation Application', 'article_2': 'MSN', 'similarity_score': 0.1}, {'article_1': 'Tesla Launching 2 e-Car Models in India, Files Homologation Application', 'article_2': \"Tesla Model 3, Model Y's Homologation Process Initiated In India\", 'similarity_score': 0.19}, {'article_1': 'Tesla Launching 2 e-Car Models in India, Files Homologation Application', 'article_2': 'Tesla Begins Certification Process for Model Y, Model 3 in India', 'similarity_score': 0.14}, {'article_1': 'Tesla Launching 2 e-Car Models in India, Files Homologation Application', 'article_2': 'Error extracting content', 'similarity_score': 0.08}, {'article_1': 'Tesla Launching 2 e-Car Models in India, Files Homologation Application', 'article_2': 'As Tesla tanks, Elon Musk’s chosen board chair thrives - Telegraph India', 'similarity_score': 0.18}, {'article_1': 'Tesla Launching 2 e-Car Models in India, Files Homologation Application', 'article_2': 'MSN', 'similarity_score': 0.1}, {'article_1': 'MSN', 'article_2': \"Tesla Model 3, Model Y's Homologation Process Initiated In India\", 'similarity_score': 0.03}, {'article_1': 'MSN', 'article_2': 'Tesla Begins Certification Process for Model Y, Model 3 in India', 'similarity_score': 0.01}, {'article_1': 'MSN', 'article_2': 'Error extracting content', 'similarity_score': 0.29}, {'article_1': 'MSN', 'article_2': 'As Tesla tanks, Elon Musk’s chosen board chair thrives - Telegraph India', 'similarity_score': 0.06}, {'article_1': 'MSN', 'article_2': 'MSN', 'similarity_score': 0.45}, {'article_1': \"Tesla Model 3, Model Y's Homologation Process Initiated In India\", 'article_2': 'Tesla Begins Certification Process for Model Y, Model 3 in India', 'similarity_score': 0.29}, {'article_1': \"Tesla Model 3, Model Y's Homologation Process Initiated In India\", 'article_2': 'Error extracting content', 'similarity_score': 0.01}, {'article_1': \"Tesla Model 3, Model Y's Homologation Process Initiated In India\", 'article_2': 'As Tesla tanks, Elon Musk’s chosen board chair thrives - Telegraph India', 'similarity_score': 0.13}, {'article_1': \"Tesla Model 3, Model Y's Homologation Process Initiated In India\", 'article_2': 'MSN', 'similarity_score': 0.03}, {'article_1': 'Tesla Begins Certification Process for Model Y, Model 3 in India', 'article_2': 'Error extracting content', 'similarity_score': 0.0}, {'article_1': 'Tesla Begins Certification Process for Model Y, Model 3 in India', 'article_2': 'As Tesla tanks, Elon Musk’s chosen board chair thrives - Telegraph India', 'similarity_score': 0.05}, {'article_1': 'Tesla Begins Certification Process for Model Y, Model 3 in India', 'article_2': 'MSN', 'similarity_score': 0.01}, {'article_1': 'Error extracting content', 'article_2': 'As Tesla tanks, Elon Musk’s chosen board chair thrives - Telegraph India', 'similarity_score': 0.03}, {'article_1': 'Error extracting content', 'article_2': 'MSN', 'similarity_score': 0.22}, {'article_1': 'As Tesla tanks, Elon Musk’s chosen board chair thrives - Telegraph India', 'article_2': 'MSN', 'similarity_score': 0.03}]}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from newspaper import Article\n",
    "import time\n",
    "import os\n",
    "import spacy\n",
    "from dotenv import load_dotenv\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.schema import Document\n",
    "from textblob import TextBlob  \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from deep_translator import GoogleTranslator\n",
    "from gtts import gTTS \n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "# Load NLP model for topic extraction\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Initialize LLM\n",
    "llm = ChatGroq(groq_api_key=api_key, model=\"Gemma2-9b-It\")\n",
    "\n",
    "def fetch_news_links_bing(company, max_articles=10):\n",
    "    \"\"\"Fetch at least 5 unique news article links from Bing News\"\"\"\n",
    "    search_url = f\"https://www.bing.com/news/search?q={company.replace(' ', '+')}\"\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "    response = requests.get(search_url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    news_links = set()\n",
    "    for link in soup.find_all(\"a\", href=True):\n",
    "        url = link[\"href\"]\n",
    "        if url.startswith(\"http\") and \"bing.com\" not in url and url not in news_links:\n",
    "            news_links.add(url)\n",
    "        if len(news_links) >= max_articles:\n",
    "            break\n",
    "\n",
    "    if len(news_links) < max_articles:\n",
    "        print(f\"Warning: Only {len(news_links)} unique articles found.\")\n",
    "    \n",
    "    return list(news_links)[:max_articles]\n",
    "\n",
    "def extract_article_text(url):\n",
    "    \"\"\"Extract clean article text using newspaper3k\"\"\"\n",
    "    try:\n",
    "        article = Article(url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        \n",
    "        return article.title, article.text if article.text else \"Could not extract article content.\"\n",
    "    except Exception as e:\n",
    "        return \"Error extracting content\", f\"Error extracting content: {str(e)}\"\n",
    "\n",
    "def extract_key_topics(text):\n",
    "    \"\"\"Extracts key topics using spaCy NLP\"\"\"\n",
    "    doc = nlp(text)\n",
    "    topics = set()\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in [\"ORG\", \"GPE\", \"EVENT\", \"PERSON\"]:\n",
    "            topics.add(ent.text)\n",
    "    \n",
    "    return list(topics)\n",
    "\n",
    "def summarize_text(speech_text):\n",
    "    \"\"\"Summarizes the article content\"\"\"\n",
    "    \n",
    "    docs = [Document(page_content=speech_text)]\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=100)\n",
    "    final_documents = text_splitter.split_documents(docs)\n",
    "\n",
    "    chunks_prompt = PromptTemplate(\n",
    "        input_variables=['text'],\n",
    "        template=\"Please summarize the below speech:\\nSpeech: `{text}`\\nSummary:\"\n",
    "    )\n",
    "\n",
    "    final_prompt_template = PromptTemplate(\n",
    "        input_variables=['text'],\n",
    "        template=\"Here is a concise summary of the speech:\\n\\n{text}\"\n",
    "    )\n",
    "\n",
    "    summary_chain = load_summarize_chain(\n",
    "        llm=llm,\n",
    "        chain_type=\"map_reduce\",\n",
    "        map_prompt=chunks_prompt,\n",
    "        combine_prompt=final_prompt_template,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    summary_output = summary_chain.invoke({\"input_documents\": final_documents})['output_text']\n",
    "    return summary_output\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    \"\"\"Performs sentiment analysis using TextBlob\"\"\"\n",
    "    sentiment_score = TextBlob(text).sentiment.polarity\n",
    "    if sentiment_score > 0:\n",
    "        return \"Positive\"\n",
    "    elif sentiment_score < 0:\n",
    "        return \"Negative\"\n",
    "    else:\n",
    "        return \"Neutral\"\n",
    "\n",
    "\n",
    "\n",
    "def compare_articles(articles):\n",
    "    texts = [article[\"Summary\"] for article in articles]\n",
    "    vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "    tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "    \n",
    "    comparison_results = []\n",
    "    for i in range(len(articles)):\n",
    "        for j in range(i + 1, len(articles)):\n",
    "            similarity = (tfidf_matrix[i].dot(tfidf_matrix[j].T)).toarray()[0, 0]\n",
    "            comparison_results.append({\n",
    "                \"article_1\": articles[i][\"Title\"],\n",
    "                \"article_2\": articles[j][\"Title\"],\n",
    "                \"similarity_score\": round(float(similarity), 2)\n",
    "            })\n",
    "    \n",
    "    return comparison_results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    company_name = input(\"Enter company name: \")\n",
    "    news_links = fetch_news_links_bing(company_name, max_articles=10)\n",
    "\n",
    "    articles_data = []\n",
    "\n",
    "    if news_links:\n",
    "        for idx, link in enumerate(news_links):\n",
    "            print(f\"\\nFetching article [{idx+1}]: {link}\")\n",
    "            time.sleep(1)\n",
    "            \n",
    "            title, article_text = extract_article_text(link)\n",
    "            summary = summarize_text(article_text)\n",
    "            key_topics = extract_key_topics(summary)\n",
    "            sentiment = analyze_sentiment(summary)\n",
    "            \n",
    "            articles_data.append({\n",
    "                \"Title\": title,\n",
    "                \"Summary\": summary,\n",
    "                \"Sentiment\": sentiment,\n",
    "                \"Topics\": key_topics\n",
    "            })\n",
    "\n",
    "    final_sentiment = compare_articles(articles_data)\n",
    "\n",
    "    output_data = {\n",
    "        \"Company\": company_name,\n",
    "        \"Articles\": articles_data,\n",
    "        \"Final Sentiment Analysis\": final_sentiment\n",
    "    }\n",
    "\n",
    "    print(\"\\n\\nFinal Output:\\n\")\n",
    "    print(output_data)\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_text = final_sentiment\n",
    "hindi_text = GoogleTranslator(source='en', target='hi').translate(english_text)\n",
    "\n",
    "print(\"Hindi Translation:\", hindi_text)\n",
    "\n",
    "\n",
    "\n",
    "tts = gTTS(text=hindi_text, lang='hi')  \n",
    "tts.save(\"hindi_audio.mp3\")  # Save the audio file  \n",
    "\n",
    "# Step 3: Play the audio file (Windows)\n",
    "os.system(\"start hindi_audio.mp3\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gtts in f:\\news_summarixation\\venv\\lib\\site-packages (2.5.4)\n",
      "Requirement already satisfied: requests<3,>=2.27 in f:\\news_summarixation\\venv\\lib\\site-packages (from gtts) (2.32.3)\n",
      "Requirement already satisfied: click<8.2,>=7.1 in f:\\news_summarixation\\venv\\lib\\site-packages (from gtts) (8.1.8)\n",
      "Requirement already satisfied: colorama in f:\\news_summarixation\\venv\\lib\\site-packages (from click<8.2,>=7.1->gtts) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in f:\\news_summarixation\\venv\\lib\\site-packages (from requests<3,>=2.27->gtts) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in f:\\news_summarixation\\venv\\lib\\site-packages (from requests<3,>=2.27->gtts) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in f:\\news_summarixation\\venv\\lib\\site-packages (from requests<3,>=2.27->gtts) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in f:\\news_summarixation\\venv\\lib\\site-packages (from requests<3,>=2.27->gtts) (2025.1.31)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gtts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: googletrans 4.0.0rc1Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Uninstalling googletrans-4.0.0rc1:\n",
      "  Successfully uninstalled googletrans-4.0.0rc1\n",
      "Found existing installation: httpcore 1.0.7\n",
      "Uninstalling httpcore-1.0.7:\n",
      "  Successfully uninstalled httpcore-1.0.7\n",
      "Found existing installation: httpx 0.28.1\n",
      "Uninstalling httpx-0.28.1:\n",
      "  Successfully uninstalled httpx-0.28.1\n"
     ]
    }
   ],
   "source": [
    "pip uninstall googletrans httpcore httpx -y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting googletrans==4.0.0-rc1\n",
      "  Using cached googletrans-4.0.0rc1-py3-none-any.whl\n",
      "Collecting httpx==0.23.0\n",
      "  Downloading httpx-0.23.0-py3-none-any.whl.metadata (52 kB)\n",
      "Collecting httpcore==0.15.0\n",
      "  Downloading httpcore-0.15.0-py3-none-any.whl.metadata (15 kB)\n",
      "INFO: pip is looking at multiple versions of googletrans to determine which version is compatible with other requirements. This could take a while.\n",
      "\n",
      "The conflict is caused by:\n",
      "    The user requested httpx==0.23.0\n",
      "    googletrans 4.0.0rc1 depends on httpx==0.13.3\n",
      "\n",
      "To fix this you could try to:\n",
      "1. loosen the range of package versions you've specified\n",
      "2. remove package versions to allow pip to attempt to solve the dependency conflict\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Cannot install googletrans==4.0.0rc1 and httpx==0.23.0 because these package versions have conflicting dependencies.\n",
      "ERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\n"
     ]
    }
   ],
   "source": [
    "pip install googletrans==4.0.0-rc1 httpx==0.23.0 httpcore==0.15.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The latest news coverage is mostly positive. Potential stock growth expected.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting deep-translator\n",
      "  Downloading deep_translator-1.11.4-py3-none-any.whl.metadata (30 kB)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in f:\\news_summarixation\\venv\\lib\\site-packages (from deep-translator) (4.13.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.23.0 in f:\\news_summarixation\\venv\\lib\\site-packages (from deep-translator) (2.32.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in f:\\news_summarixation\\venv\\lib\\site-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (2.6)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in f:\\news_summarixation\\venv\\lib\\site-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in f:\\news_summarixation\\venv\\lib\\site-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in f:\\news_summarixation\\venv\\lib\\site-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in f:\\news_summarixation\\venv\\lib\\site-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in f:\\news_summarixation\\venv\\lib\\site-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2025.1.31)\n",
      "Downloading deep_translator-1.11.4-py3-none-any.whl (42 kB)\n",
      "Installing collected packages: deep-translator\n",
      "Successfully installed deep-translator-1.11.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install deep-translator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching article [1]: https://www.timesnownews.com/auto/electric-vehicles/tesla-begins-certification-process-for-model-y-model-3-in-india-article-119120768\n",
      "\n",
      "Fetching article [2]: https://www.msn.com/en-us/money/companies/tesla-investor-christopher-tsai-hopes-musk-s-doge-role-is-short-lived/ar-AA1AZsHA?ocid=BingNewsVerp\n",
      "\n",
      "Fetching article [3]: https://www.indianweb2.com/2025/03/tesla-launching-2-e-car-models-in-india.html\n",
      "\n",
      "Fetching article [4]: https://edition.cnn.com/2025/03/15/business/elon-musk-tesla-demonstrations-doge/index.html\n",
      "\n",
      "Fetching article [5]: https://www.ndtv.com/auto/tesla-model-3-model-ys-homologation-process-initiated-in-india-7940909\n",
      "\n",
      "Fetching article [6]: https://www.msn.com/en-in/entertainment/bollywood/as-tesla-tanks-musk-s-chosen-board-chair-stands-strong-as-a-badass-woman-in-business-world/ar-AA1B4QO5?ocid=BingNewsVerp\n",
      "\n",
      "Fetching article [7]: https://www.reuters.com/investigations/tesla-tanks-musks-hand-picked-board-chair-is-doing-just-fine-2025-03-17/\n",
      "\n",
      "Fetching article [8]: https://indianexpress.com/article/world/pictures-hundreds-protest-tesla-showrooms-over-musk-doge-role-9888446/\n",
      "\n",
      "Fetching article [9]: https://www.telegraphindia.com/world/as-tesla-tanks-elon-musks-chosen-board-chair-thrives/cid/2089317\n",
      "\n",
      "Fetching article [10]: https://www.msn.com/en-in/news/other/internet-slams-kim-kardashian-s-tesla-photoshoot-as-anti-musk-protests-gain-momentum/ar-AA1B26lN?ocid=BingNewsVerp\n",
      "\n",
      "\n",
      "Final Output:\n",
      "\n",
      "{'Company': 'tesla', 'Articles': [{'Title': 'Tesla Begins Certification Process for Model Y, Model 3 in India', 'Summary': \"Tesla has officially begun the process of selling its Model Y and Model 3 cars in India by applying for homologation, a crucial certification for vehicle roadworthiness. This move comes amidst ongoing free trade agreement discussions between the US and India, which could potentially lower tariffs.\\n\\nWhile Elon Musk desires a foothold in the burgeoning Indian electric vehicle market, he initially aims to export cars rather than establish local manufacturing, despite the Indian government's push for local production. \\n\\nIndia's electric car market experienced a 20% surge in 2024, with Tata Motors and JSW MG Motors leading the pack. The luxury segment also witnessed growth, with brands like BMW and Mercedes-Benz seeing increased sales. \\n\\n\\nThis development signifies Tesla's serious intent to enter the Indian market, but the future of local manufacturing remains uncertain.\", 'Sentiment': 'Positive', 'Topics': ['India', 'Elon Musk', 'Tata Motors', 'US', 'Tesla', 'BMW', 'JSW MG Motors', 'Mercedes-Benz']}, {'Title': 'Error extracting content', 'Summary': 'Please provide the article content so I can summarize it for you. \\n\\nI need the text of the article to be able to understand it and create a summary.', 'Sentiment': 'Positive', 'Topics': []}, {'Title': 'Tesla Launching 2 e-Car Models in India, Files Homologation Application', 'Summary': 'This article compares the Tesla Model 3 and Model Y, highlighting their key features. \\n\\n**Tesla Model 3:**\\n\\n* **Range:** Up to 363 miles \\n* **Performance:** Quick acceleration with dual-motor all-wheel drive.\\n* **Interior:** Minimalist design with a central touchscreen, premium materials, and ambient lighting.\\n* **Technology:** Advanced driver-assistance features, over-the-air software updates, and acoustic glass for a quieter cabin.\\n* **Charging:** Fast charging with Tesla Superchargers.\\n* **Comfort:** Ventilated seats, wireless charging, and a rear passenger display.\\n\\n**Tesla Model Y:**\\n\\n* **Range:** Up to 337 miles\\n* **Performance:** Dual-motor all-wheel drive for handling various road conditions.\\n* **Safety:** High safety ratings with features like Forward Collision Warning and Active Emergency Braking.\\n* **Interior:** Spacious with fold-flat seats, two trunks, and a 15-inch touchscreen.\\n* **Cargo Space:** Up to 76 cubic feet.\\n* **Versatility:** Suitable for road trips and daily commutes with fast charging and towing capabilities.\\n\\nBoth models prioritize efficiency, safety, and a smooth driving experience. The Model 3 is known for its sporty performance and sleek design, while the Model Y offers more spaciousness and versatility.', 'Sentiment': 'Positive', 'Topics': ['Tesla Superchargers', 'Forward Collision Warning and Active Emergency Braking']}, {'Title': 'Hundreds gather outside Tesla showrooms in backlash to Elon Musk’s role with DOGE', 'Summary': 'Hundreds of protesters gathered outside Tesla showrooms across the US on Saturday as part of the \"Tesla Takedown\" movement, a boycott against CEO Elon Musk. \\n\\nThe demonstration is fueled by criticism of Musk\\'s role in the Department of Government Efficiency (DOGE), which has cut thousands of government jobs and proposed significant IRS downsizing. \\n\\nProtesters, calling for people to \"sell your Teslas, dump your stock, join the picket lines,\" organized over 80 demonstrations on Saturday and plan more through April.  \\n\\nMajor cities like Baltimore, Dedham, West Chester, and Washington, D.C. saw significant turnout, with some locations drawing over 300 people. The protests featured signs, music, and dancing, with some organizers aiming to create a festive atmosphere.  The movement, started by actor Alex Winter and professor Joan Donovan, aims to pressure Musk and Tesla through consumer action.', 'Sentiment': 'Positive', 'Topics': ['West Chester', 'Alex Winter', 'Elon Musk', 'Musk', 'IRS', 'US', 'DOGE', 'Dedham', 'Teslas', 'Joan Donovan', 'Tesla', 'Washington', 'the Department of Government Efficiency', 'the \"Tesla Takedown', 'D.C.', 'Baltimore']}, {'Title': \"Tesla Model 3, Model Y's Homologation Process Initiated In India\", 'Summary': \"Tesla has officially begun the process of selling its electric cars in India. The company has submitted applications to homologate (certify roadworthiness) its Model Y and Model 3 vehicles. This crucial step comes as Tesla finalizes its first showroom in India and ramps up hiring. \\n\\nThe timing is significant as it coincides with ongoing discussions for a free trade agreement between India and the US, which could potentially reduce tariffs and benefit Tesla's entry into the Indian market. Tesla's arrival in India has been delayed for over three years due to disagreements over trade terms.\", 'Sentiment': 'Positive', 'Topics': ['India', 'US', 'Tesla']}, {'Title': 'Error extracting content', 'Summary': 'Please provide the article content so I can summarize it for you.', 'Sentiment': 'Neutral', 'Topics': []}, {'Title': 'In pictures: Hundreds protest outside Tesla showrooms over Musk’s role in DOGE', 'Summary': 'Hundreds of protesters demonstrated outside Tesla showrooms across the United States on Saturday, criticizing Elon Musk\\'s role in the Department of Government Efficiency (DOGE) and its proposed cost-cutting measures, including a significant reduction in the IRS workforce.\\n\\nThe protests, organized by the \"Tesla Takedown\" movement, urge people to divest from Tesla and join the movement against DOGE\\'s policies. Demonstrations occurred in cities like Dedham (MA), West Chester (PA), Baltimore (MD), and Washington D.C., with hundreds of participants in some locations. \\n\\nThe movement highlights concerns over DOGE\\'s impact on public services and its potential to disproportionately affect vulnerable populations.  \\n\\nAdding another layer to the situation, Attorney General Pam Bondi announced plans to address recent vandalism targeting Tesla properties.', 'Sentiment': 'Negative', 'Topics': ['West Chester', 'the United States', \"Elon Musk's\", 'IRS', 'Pam Bondi', 'DOGE', 'Tesla', 'Dedham', 'MD', 'Washington D.C.', 'the Department of Government Efficiency', 'the \"Tesla Takedown', 'Baltimore', 'MA']}, {'Title': 'As Tesla tanks, Elon Musk’s chosen board chair thrives - Telegraph India', 'Summary': \"This article highlights the stark contrast in fortunes between Tesla CEO Elon Musk and its Chair, Robyn Denholm. \\n\\nWhile Musk faces political backlash for his controversial stances and Tesla's stock plummets, Denholm enjoys lavish compensation, making her the highest-paid chair of any U.S. public company. \\n\\nHer total compensation since 2014 has reached $682 million, primarily due to the surge in Tesla's stock value. This wealth has attracted scrutiny, leading to a shareholder lawsuit that forced her to return some funds and a court case questioning her approval of Musk's record-breaking $56 billion compensation package. \\n\\nDespite the criticism, Denholm remains a staunch defender of the pay structure, highlighting the positive impact on her own life. \\n\\n\\nThis article exposes the ethical dilemma surrounding executive compensation, especially during a time when the company faces public scrutiny and financial instability.\", 'Sentiment': 'Positive', 'Topics': ['Elon Musk', 'Musk', 'Chair', 'Tesla', 'Denholm', 'U.S.']}, {'Title': 'Error extracting content', 'Summary': 'Please provide me with the article you would like me to summarize. I need the text content to be able to create a summary. \\n\\n\\nLet me know if you have any other questions!', 'Sentiment': 'Positive', 'Topics': []}], 'Final Sentiment Analysis': 'Sentiment Analysis Summary for tesla News Coverage:\\n    - Positive Articles: 7 (77.78%)\\n    - Negative Articles: 1 (11.11%)\\n    - Neutral Articles: 1 (11.11%)\\n\\n    Insights:\\n    - The majority of news articles are positive.\\n    - There is 66.67% difference between positive and negative coverage.\\n    - This analysis suggests that media perception of tesla is generally favorable.'}\n",
      "Hindi Translation: टेस्ला समाचार कवरेज के लिए भावना विश्लेषण सारांश:\n",
      "    - सकारात्मक लेख: 7 (77.78%)\n",
      "    - नकारात्मक लेख: 1 (11.11%)\n",
      "    - तटस्थ लेख: 1 (11.11%)\n",
      "\n",
      "    अंतर्दृष्टि:\n",
      "    - अधिकांश समाचार लेख सकारात्मक हैं।\n",
      "    - सकारात्मक और नकारात्मक कवरेज के बीच 66.67% अंतर है।\n",
      "    - इस विश्लेषण से पता चलता है कि टेस्ला की मीडिया धारणा आम तौर पर अनुकूल है।\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from newspaper import Article\n",
    "import time\n",
    "import os\n",
    "import spacy\n",
    "from dotenv import load_dotenv\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.schema import Document\n",
    "from textblob import TextBlob  \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from deep_translator import GoogleTranslator\n",
    "from gtts import gTTS\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "# Load NLP model for topic extraction\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Initialize LLM\n",
    "llm = ChatGroq(groq_api_key=api_key, model=\"Gemma2-9b-It\")\n",
    "\n",
    "def fetch_news_links_bing(company, max_articles=5):\n",
    "    \"\"\"Fetch at least 5 unique news article links from Bing News\"\"\"\n",
    "    search_url = f\"https://www.bing.com/news/search?q={company.replace(' ', '+')}\"\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "    response = requests.get(search_url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    news_links = set()\n",
    "    base_url = \"https://www.bing.com\"\n",
    "\n",
    "    for link in soup.find_all(\"a\", href=True):\n",
    "        url = urljoin(base_url, link[\"href\"])  # Convert to absolute URL\n",
    "        if url.startswith(\"http\") and \"bing.com\" not in url and url not in news_links:\n",
    "            news_links.add(url)\n",
    "        if len(news_links) >= max_articles:\n",
    "            break\n",
    "\n",
    "    if len(news_links) < max_articles:\n",
    "        print(f\"Warning: Only {len(news_links)} unique articles found.\")\n",
    "    \n",
    "    return list(news_links)[:max_articles]\n",
    "\n",
    "def extract_article_text(url):\n",
    "    \"\"\"Extract clean article text using newspaper3k\"\"\"\n",
    "    try:\n",
    "        article = Article(url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "\n",
    "        if not article.text.strip():\n",
    "            return \"Error extracting content\", \"No content extracted.\"\n",
    "\n",
    "        return article.title, article.text\n",
    "    except Exception as e:\n",
    "        return \"Error extracting content\", f\"Error extracting content: {str(e)}\"\n",
    "\n",
    "def extract_key_topics(text):\n",
    "    \"\"\"Extracts key topics using spaCy NLP\"\"\"\n",
    "    doc = nlp(text)\n",
    "    topics = {ent.text for ent in doc.ents if ent.label_ in [\"ORG\", \"GPE\", \"EVENT\", \"PERSON\"]}\n",
    "    return list(topics)\n",
    "\n",
    "def summarize_text(article_text):\n",
    "    \"\"\"Summarizes the article content using LLM\"\"\"\n",
    "    \n",
    "    if not article_text.strip():\n",
    "        return \"Summary not available due to extraction failure.\"\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=100)\n",
    "    docs = text_splitter.split_documents([Document(page_content=article_text)])\n",
    "\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=['text'],\n",
    "        template=\"Summarize the following article:\\n{text}\"\n",
    "    )\n",
    "\n",
    "    summary_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "    summary_output = summary_chain.run({\"text\": docs[0].page_content})  # Summarizing first chunk\n",
    "    return summary_output.strip() if summary_output else \"Summary not generated.\"\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    \"\"\"Performs sentiment analysis using TextBlob\"\"\"\n",
    "    sentiment_score = TextBlob(text).sentiment.polarity\n",
    "    return \"Positive\" if sentiment_score > 0 else \"Negative\" if sentiment_score < 0 else \"Neutral\"\n",
    "\n",
    "def generate_sentiment_summary(articles_data):\n",
    "    \"\"\"Generates summary insights based on sentiment analysis\"\"\"\n",
    "    total_articles = len(articles_data)\n",
    "    \n",
    "    if total_articles == 0:\n",
    "        return \"No valid articles were processed for analysis.\"\n",
    "\n",
    "    sentiment_counts = {\"Positive\": 0, \"Negative\": 0, \"Neutral\": 0}\n",
    "\n",
    "    for article in articles_data:\n",
    "        sentiment_counts[article[\"Sentiment\"]] += 1\n",
    "\n",
    "    # Calculate percentages\n",
    "    positive_pct = round((sentiment_counts[\"Positive\"] / total_articles) * 100, 2)\n",
    "    negative_pct = round((sentiment_counts[\"Negative\"] / total_articles) * 100, 2)\n",
    "    neutral_pct = round((sentiment_counts[\"Neutral\"] / total_articles) * 100, 2)\n",
    "\n",
    "    summary = f\"\"\"\n",
    "    Sentiment Analysis Summary for {company_name} News Coverage:\n",
    "    - Positive Articles: {sentiment_counts['Positive']} ({positive_pct}%)\n",
    "    - Negative Articles: {sentiment_counts['Negative']} ({negative_pct}%)\n",
    "    - Neutral Articles: {sentiment_counts['Neutral']} ({neutral_pct}%)\n",
    "\n",
    "    Insights:\n",
    "    - The majority of news articles are {'positive' if positive_pct > negative_pct else 'negative' if negative_pct > positive_pct else 'neutral'}.\n",
    "    - There is {abs(positive_pct - negative_pct)}% difference between positive and negative coverage.\n",
    "    - This analysis suggests that media perception of {company_name} is {('generally favorable' if positive_pct > negative_pct else 'somewhat critical' if negative_pct > positive_pct else 'balanced')}.\n",
    "    \"\"\"\n",
    "    return summary.strip()\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    company_name = input(\"Enter company name: \")\n",
    "    news_links = fetch_news_links_bing(company_name, max_articles=10)\n",
    "\n",
    "    articles_data = []\n",
    "\n",
    "    if news_links:\n",
    "        for idx, link in enumerate(news_links):\n",
    "            print(f\"\\nFetching article [{idx+1}]: {link}\")\n",
    "            time.sleep(1)\n",
    "            \n",
    "            title, article_text = extract_article_text(link)\n",
    "            if article_text.startswith(\"Error\"):\n",
    "                continue  # Skip faulty articles\n",
    "            \n",
    "            summary = summarize_text(article_text)\n",
    "            key_topics = extract_key_topics(summary)\n",
    "            sentiment = analyze_sentiment(summary)\n",
    "            \n",
    "            articles_data.append({\n",
    "                \"Title\": title,\n",
    "                \"Summary\": summary,\n",
    "                \"Sentiment\": sentiment,\n",
    "                \"Topics\": key_topics\n",
    "            })\n",
    "\n",
    "    sentiment_summary = generate_sentiment_summary(articles_data)\n",
    "    output_data = {\n",
    "        \"Company\": company_name,\n",
    "        \"Articles\": articles_data,\n",
    "        \"Final Sentiment Analysis\":sentiment_summary\n",
    "        \n",
    "    }\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    print(\"\\n\\nFinal Output:\\n\")\n",
    "    print(output_data)\n",
    "    \n",
    "    english_text = sentiment_summary\n",
    "    hindi_text = GoogleTranslator(source='en', target='hi').translate(english_text)\n",
    "\n",
    "    print(\"Hindi Translation:\", hindi_text)\n",
    "\n",
    "\n",
    "\n",
    "    tts = gTTS(text=hindi_text, lang='hi')  \n",
    "    tts.save(\"hindi_audio.mp3\")  # Save the audio file  \n",
    "\n",
    "    # Step 3: Play the audio file (Windows)\n",
    "    os.system(\"start hindi_audio.mp3\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hindi Translation: टेस्ला समाचार कवरेज के लिए भावना विश्लेषण सारांश:\n",
      "    - सकारात्मक लेख: 8 (100.0%)\n",
      "    - नकारात्मक लेख: 0 (0.0%)\n",
      "    - तटस्थ लेख: 0 (0.0%)\n",
      "\n",
      "    अंतर्दृष्टि:\n",
      "    - अधिकांश समाचार लेख सकारात्मक हैं।\n",
      "    - सकारात्मक और नकारात्मक कवरेज के बीच 100.0% अंतर है।\n",
      "    - इस विश्लेषण से पता चलता है कि टेस्ला की मीडिया धारणा ** आम तौर पर अनुकूल है **।\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_text = sentiment_summary\n",
    "hindi_text = GoogleTranslator(source='en', target='hi').translate(english_text)\n",
    "\n",
    "print(\"Hindi Translation:\", hindi_text)\n",
    "\n",
    "\n",
    "\n",
    "tts = gTTS(text=hindi_text, lang='hi')  \n",
    "tts.save(\"hindi_audio.mp3\")  # Save the audio file  \n",
    "\n",
    "# Step 3: Play the audio file (Windows)\n",
    "os.system(\"start hindi_audio.mp3\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching article [1]: https://www.livemint.com/global/the-reason-tesla-doesn-t-pay-taxes-11742180949384.html\n",
      "\n",
      "Fetching article [2]: https://www.msn.com/en-in/autos/news/never-physically-hurt-anyone-elon-musk-reacts-to-tesla-coming-under-attack/ar-AA1B60Am?ocid=BingNewsVerp\n",
      "\n",
      "Fetching article [3]: https://www.msn.com/en-in/autos/news/a-mechanical-engineer-on-twitter-claims-facebook-link-in-tesla-protests-elon-musk-responds/ar-AA1B6xlH?ocid=BingNewsVerp\n",
      "\n",
      "Fetching article [4]: https://www.theverge.com/tesla/631308/mark-rober-tesla-youtube-autopilot-lidar-fake-claims\n",
      "\n",
      "Fetching article [5]: https://www.financialexpress.com/auto/car-news/tesla-allegedly-exploiting-canadas-ev-rebate-program-all-you-need-to-know/3779345/\n",
      "\n",
      "Fetching article [6]: https://www.ndtv.com/auto/tesla-model-3-model-ys-homologation-process-initiated-in-india-7940909\n",
      "\n",
      "Fetching article [7]: https://www.msn.com/en-in/autos/news/elon-musk-confirms-tesla-s-entry-into-india-as-soon-as-humanly-possible/ar-AA1B6msj?ocid=BingNewsVerp\n",
      "\n",
      "Fetching article [8]: https://www.dailymail.co.uk/news/article-14509025/Tesla-Avi-Ben-Hamo-Cybertruck-swastika-Michael-Lewis.html\n",
      "\n",
      "Fetching article [9]: https://www.dailymail.co.uk/sciencetech/article-14506405/Elon-Musk-HALT-Cybertruck-deliveries.html\n",
      "\n",
      "Fetching article [10]: https://www.business-standard.com/world-news/toronto-excludes-tesla-vehicles-from-ev-incentive-due-to-us-trade-war-125031800134_1.html\n",
      "\n",
      "\n",
      "Final Output:\n",
      "\n",
      "{'Company': 'tesla', 'Articles': [{'Title': 'The reason Tesla doesn’t pay taxes', 'Summary': \"The article criticizes Democrats' accusations that Tesla, led by Elon Musk, is avoiding taxes by taking advantage of government incentives. \\n\\nWhile a study by the left-leaning Institute on Taxation and Economic Policy claims Tesla paid minimal federal income tax, the article argues that these are legitimate deductions for research & development and investments encouraged by Congress, particularly in green energy.\\n\\nThe article highlights that Tesla's profits stem not just from EV sales but also from regulatory credits earned due to government EV mandates and high interest income resulting from inflation under the Biden administration. Additionally, Tesla's gains from Bitcoin price appreciation are considered unrealized capital gains.\\n\\nEssentially, the article defends Tesla's tax practices as lawful and a result of government policies rather than corporate tax evasion, refuting the Democrats' claims.\", 'Sentiment': 'Negative', 'Topics': ['Elon Musk', 'Biden', 'EV', 'Congress', 'Tesla', 'Institute on Taxation and Economic Policy']}, {'Title': 'Error extracting content', 'Summary': 'Please provide the article you would like me to summarize. I need the text content to be able to create a summary for you.', 'Sentiment': 'Positive', 'Topics': []}, {'Title': 'Error extracting content', 'Summary': 'Please provide me with the article content so I can summarize it for you. \\n\\nI need the text of the article to be able to understand it and create a summary. 😊', 'Sentiment': 'Positive', 'Topics': []}, {'Title': 'Mark Rober’s Tesla video was more than a little weird', 'Summary': 'This article discusses the viral video by YouTuber Mark Rober comparing Tesla\\'s Autopilot system to a lidar-equipped vehicle, where the Tesla crashes into a wall while the other vehicle stops. \\n\\nThe author acknowledges the video\\'s popularity and the criticism it received, including accusations of fakery and a possible agenda against Tesla. They also point out that the title of the video, \"Can you fool a self-driving car,\" is misleading as neither vehicle was truly self-driving, and Tesla\\'s Autopilot requires driver engagement.\\n\\nThe author emphasizes the need to differentiate between advanced driver-assistance systems like Autopilot and fully self-driving vehicles, highlighting a common misconception. They intend to further address other criticisms raised against the video.', 'Sentiment': 'Negative', 'Topics': ['Autopilot', 'Mark Rober', 'Tesla']}, {'Title': 'Tesla allegedly exploiting Canada’s EV rebate program: All you need to know', 'Summary': \"This article reports that Tesla may have exploited Canada's EV rebate program. Before the program was discontinued for higher-priced EVs, Tesla saw a massive spike in sales, particularly in British Columbia, securing approximately $90 million in government subsidies.\\n\\nThe surge in sales, particularly a three-day period in January with over 8,600 vehicle registrations, led to an investigation by Canadian officials. Industry stakeholders, like the Canadian Automobile Dealers Association, believe Tesla strategically timed these registrations to maximize their rebate earnings before the program's end.\\n\\nThis incident comes amidst escalating trade tensions between Canada and the U.S., with Canada implementing countermeasures like banning Tesla charger rebates in response to U.S. tariffs. Tesla's stock price has also been declining, adding to the controversy. \\n\\n\\nThe article highlights concerns about potential abuse of government programs and the impact of trade tensions on the automotive industry.\", 'Sentiment': 'Negative', 'Topics': ['the Canadian Automobile Dealers Association', 'U.S.', 'EV', 'Tesla', 'Canada', 'British Columbia']}, {'Title': \"Tesla Model 3, Model Y's Homologation Process Initiated In India\", 'Summary': \"Tesla has officially begun the process of selling its electric cars in India. \\n\\nThey have submitted applications to homologate the Model 3 and Model Y, a crucial step ensuring the vehicles meet Indian safety and emission standards. This move comes as Tesla finalizes its first showroom in India and begins hiring local staff. \\n\\nThe timing is significant as it coincides with negotiations for a free trade agreement between India and the US, which could potentially lower tariffs and benefit Tesla's entry into the market. Tesla's arrival in India has been delayed for over three years due to disagreements over trade terms.\", 'Sentiment': 'Positive', 'Topics': ['US', 'Tesla', 'India']}, {'Title': 'Error extracting content', 'Summary': 'Please provide the article content so I can summarize it for you. I need the text of the article to be able to understand and summarize it.', 'Sentiment': 'Positive', 'Topics': []}, {'Title': 'Error extracting content', 'Summary': 'Please provide the article content so I can summarize it for you.  I need the text of the article to be able to understand and summarize it.', 'Sentiment': 'Positive', 'Topics': []}], 'Final Sentiment Analysis': 'Sentiment Analysis Summary for tesla News Coverage:\\n    - Positive Articles: 5 (62.5%)\\n    - Negative Articles: 3 (37.5%)\\n    - Neutral Articles: 0 (0.0%)\\n\\n    Insights:\\n    - The majority of news articles are positive.\\n    - There is 25.0% difference between positive and negative coverage.\\n    - This analysis suggests that media perception of tesla is generally favorable.'}\n",
      "Hindi Translation: टेस्ला समाचार कवरेज के लिए भावना विश्लेषण सारांश:\n",
      "    - सकारात्मक लेख: 5 (62.5%)\n",
      "    - नकारात्मक लेख: 3 (37.5%)\n",
      "    - तटस्थ लेख: 0 (0.0%)\n",
      "\n",
      "    अंतर्दृष्टि:\n",
      "    - अधिकांश समाचार लेख सकारात्मक हैं।\n",
      "    - सकारात्मक और नकारात्मक कवरेज के बीच 25.0% अंतर है।\n",
      "    - इस विश्लेषण से पता चलता है कि टेस्ला की मीडिया धारणा आम तौर पर अनुकूल है।\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from newspaper import Article\n",
    "import time\n",
    "import os\n",
    "import spacy\n",
    "from dotenv import load_dotenv\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.schema import Document\n",
    "from textblob import TextBlob  \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from deep_translator import GoogleTranslator\n",
    "from gtts import gTTS\n",
    "from urllib.parse import urljoin\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain, create_history_aware_retriever\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "# Load NLP model for topic extraction\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Initialize LLM\n",
    "llm = ChatGroq(groq_api_key=api_key, model=\"Gemma2-9b-It\")\n",
    "\n",
    "def fetch_news_links_bing(company, max_articles=10):\n",
    "    \"\"\"Fetch at least 5 unique news article links from Bing News\"\"\"\n",
    "    search_url = f\"https://www.bing.com/news/search?q={company.replace(' ', '+')}\"\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "    response = requests.get(search_url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    news_links = set()\n",
    "    base_url = \"https://www.bing.com\"\n",
    "\n",
    "    for link in soup.find_all(\"a\", href=True):\n",
    "        url = urljoin(base_url, link[\"href\"])  # Convert to absolute URL\n",
    "        if url.startswith(\"http\") and \"bing.com\" not in url and url not in news_links:\n",
    "            news_links.add(url)\n",
    "        if len(news_links) >= max_articles:\n",
    "            break\n",
    "\n",
    "    if len(news_links) < max_articles:\n",
    "        print(f\"Warning: Only {len(news_links)} unique articles found.\")\n",
    "    \n",
    "    return list(news_links)[:max_articles]\n",
    "\n",
    "def extract_article_text(url):\n",
    "    \"\"\"Extract clean article text using newspaper3k\"\"\"\n",
    "    try:\n",
    "        article = Article(url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "\n",
    "        if not article.text.strip():\n",
    "            return \"Error extracting content\", \"No content extracted.\"\n",
    "\n",
    "        return article.title, article.text\n",
    "    except Exception as e:\n",
    "        return \"Error extracting content\", f\"Error extracting content: {str(e)}\"\n",
    "\n",
    "\n",
    "\n",
    "# Function to process PDF files and answer queries\n",
    "def process_query(uploaded_files, query):\n",
    "    if uploaded_files:\n",
    "        contents = uploaded_files\n",
    "        \n",
    "        # Split the text into chunks\n",
    "        text_splitter = CharacterTextSplitter(separator = \"\\n\",chunk_size = 800,chunk_overlap  = 200,length_function = len)\n",
    "        split_texts = text_splitter.split_text(contents)\n",
    "\n",
    "        embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "        db = FAISS.from_texts(split_texts, embeddings)\n",
    "\n",
    "        retriever = db.as_retriever()\n",
    "        model = ChatGroq(model=\"Gemma2-9b-It\", groq_api_key=api_key)\n",
    "\n",
    "        # Contextualization prompt\n",
    "        contextualize_q_system_prompt = (\n",
    "            \"Given a chat history and the latest user question \"\n",
    "            \"which might reference context in the chat history, \"\n",
    "            \"formulate a standalone question which can be understood \"\n",
    "            \"without the chat history. Do NOT answer the question, \"\n",
    "            \"just reformulate it if needed and otherwise return it as is.\"\n",
    "        )\n",
    "        contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\"system\", contextualize_q_system_prompt),\n",
    "                MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "                (\"human\", \"{input}\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        history_aware_retriever = create_history_aware_retriever(model, retriever, contextualize_q_prompt)\n",
    "\n",
    "        # System prompt for answering questions\n",
    "        system_prompt = (\n",
    "            \"You are an AI assistant that helps summarize and answer questions from documents.\\n\\n\"\n",
    "            \"Context:\\n{context}\\n\\n\"\n",
    "            \"Chat History:\\n{chat_history}\\n\\n\"\n",
    "            \"User Question:\\n{input}\"\n",
    "        )\n",
    "\n",
    "        qa_prompt = ChatPromptTemplate.from_template(system_prompt)\n",
    "\n",
    "        question_answer_chain = create_stuff_documents_chain(model, qa_prompt)\n",
    "        rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "\n",
    "        chat_history = []\n",
    "        response = rag_chain.invoke({\"input\": query, \"chat_history\": chat_history})\n",
    "\n",
    "        return response['answer']\n",
    "\n",
    "def extract_key_topics(text):\n",
    "    \"\"\"Extracts key topics using spaCy NLP\"\"\"\n",
    "    doc = nlp(text)\n",
    "    topics = {ent.text for ent in doc.ents if ent.label_ in [\"ORG\", \"GPE\", \"EVENT\", \"PERSON\"]}\n",
    "    return list(topics)\n",
    "\n",
    "def summarize_text(article_text):\n",
    "    \"\"\"Summarizes the article content using LLM\"\"\"\n",
    "    \n",
    "    if not article_text.strip():\n",
    "        return \"Summary not available due to extraction failure.\"\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=100)\n",
    "    docs = text_splitter.split_documents([Document(page_content=article_text)])\n",
    "\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=['text'],\n",
    "        template=\"Summarize the following article:\\n{text}\"\n",
    "    )\n",
    "\n",
    "    summary_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "    summary_output = summary_chain.run({\"text\": docs[0].page_content})  # Summarizing first chunk\n",
    "    return summary_output.strip() if summary_output else \"Summary not generated.\"\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    \"\"\"Performs sentiment analysis using TextBlob\"\"\"\n",
    "    sentiment_score = TextBlob(text).sentiment.polarity\n",
    "    return \"Positive\" if sentiment_score > 0 else \"Negative\" if sentiment_score < 0 else \"Neutral\"\n",
    "\n",
    "def generate_sentiment_summary(articles_data):\n",
    "    \"\"\"Generates summary insights based on sentiment analysis\"\"\"\n",
    "    total_articles = len(articles_data)\n",
    "    \n",
    "    if total_articles == 0:\n",
    "        return \"No valid articles were processed for analysis.\"\n",
    "\n",
    "    sentiment_counts = {\"Positive\": 0, \"Negative\": 0, \"Neutral\": 0}\n",
    "\n",
    "    for article in articles_data:\n",
    "        sentiment_counts[article[\"Sentiment\"]] += 1\n",
    "\n",
    "    # Calculate percentages\n",
    "    positive_pct = round((sentiment_counts[\"Positive\"] / total_articles) * 100, 2)\n",
    "    negative_pct = round((sentiment_counts[\"Negative\"] / total_articles) * 100, 2)\n",
    "    neutral_pct = round((sentiment_counts[\"Neutral\"] / total_articles) * 100, 2)\n",
    "\n",
    "    summary = f\"\"\"\n",
    "    Sentiment Analysis Summary for {company_name} News Coverage:\n",
    "    - Positive Articles: {sentiment_counts['Positive']} ({positive_pct}%)\n",
    "    - Negative Articles: {sentiment_counts['Negative']} ({negative_pct}%)\n",
    "    - Neutral Articles: {sentiment_counts['Neutral']} ({neutral_pct}%)\n",
    "\n",
    "    Insights:\n",
    "    - The majority of news articles are {'positive' if positive_pct > negative_pct else 'negative' if negative_pct > positive_pct else 'neutral'}.\n",
    "    - There is {abs(positive_pct - negative_pct)}% difference between positive and negative coverage.\n",
    "    - This analysis suggests that media perception of {company_name} is {('generally favorable' if positive_pct > negative_pct else 'somewhat critical' if negative_pct > positive_pct else 'balanced')}.\n",
    "    \"\"\"\n",
    "    return summary.strip()\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    company_name = input(\"Enter company name: \")\n",
    "    news_links = fetch_news_links_bing(company_name, max_articles=10)\n",
    "\n",
    "    articles_data = []\n",
    "    articals_text=[]\n",
    "    texts=' '.join(articals_text)\n",
    "\n",
    "    if news_links:\n",
    "        for idx, link in enumerate(news_links):\n",
    "            print(f\"\\nFetching article [{idx+1}]: {link}\")\n",
    "            time.sleep(1)\n",
    "            \n",
    "            title, article_text = extract_article_text(link)\n",
    "            if article_text.startswith(\"Error\"):\n",
    "                continue  # Skip faulty articles\n",
    "            articals_text.append(article_text)\n",
    "            summary = summarize_text(article_text)\n",
    "            key_topics = extract_key_topics(summary)\n",
    "            sentiment = analyze_sentiment(summary)\n",
    "            \n",
    "            articles_data.append({\n",
    "                \"Title\": title,\n",
    "                \"Summary\": summary,\n",
    "                \"Sentiment\": sentiment,\n",
    "                \"Topics\": key_topics\n",
    "            })\n",
    "\n",
    "    sentiment_summary = generate_sentiment_summary(articles_data)\n",
    "    output_data = {\n",
    "        \"Company\": company_name,\n",
    "        \"Articles\": articles_data,\n",
    "        \"Final Sentiment Analysis\":sentiment_summary\n",
    "        \n",
    "    }\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    print(\"\\n\\nFinal Output:\\n\")\n",
    "    print(output_data)\n",
    "    \n",
    "    english_text = sentiment_summary\n",
    "    hindi_text = GoogleTranslator(source='en', target='hi').translate(english_text)\n",
    "\n",
    "    print(\"Hindi Translation:\", hindi_text)\n",
    "\n",
    "\n",
    "\n",
    "    tts = gTTS(text=hindi_text, lang='hi')  \n",
    "    tts.save(\"hindi_audio.mp3\")  # Save the audio file  \n",
    "\n",
    "    # Step 3: Play the audio file (Windows)\n",
    "    os.system(\"start hindi_audio.mp3\") \n",
    "    query=input(\"please enter your query related to artical\")\n",
    "    query_process=process_query( texts, query)\n",
    "    print(query_process)\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching article [1]: https://www.livemint.com/global/the-reason-tesla-doesn-t-pay-taxes-11742180949384.html\n",
      "\n",
      "Fetching article [2]: https://www.msn.com/en-in/autos/news/never-physically-hurt-anyone-elon-musk-reacts-to-tesla-coming-under-attack/ar-AA1B60Am?ocid=BingNewsVerp\n",
      "\n",
      "Fetching article [3]: https://www.msn.com/en-in/autos/news/a-mechanical-engineer-on-twitter-claims-facebook-link-in-tesla-protests-elon-musk-responds/ar-AA1B6xlH?ocid=BingNewsVerp\n",
      "\n",
      "Fetching article [4]: https://www.theverge.com/tesla/631308/mark-rober-tesla-youtube-autopilot-lidar-fake-claims\n",
      "\n",
      "Fetching article [5]: https://www.financialexpress.com/auto/car-news/tesla-allegedly-exploiting-canadas-ev-rebate-program-all-you-need-to-know/3779345/\n",
      "\n",
      "Fetching article [6]: https://www.ndtv.com/auto/tesla-model-3-model-ys-homologation-process-initiated-in-india-7940909\n",
      "\n",
      "Fetching article [7]: https://www.msn.com/en-in/autos/news/elon-musk-confirms-tesla-s-entry-into-india-as-soon-as-humanly-possible/ar-AA1B6msj?ocid=BingNewsVerp\n",
      "\n",
      "Fetching article [8]: https://www.dailymail.co.uk/news/article-14509025/Tesla-Avi-Ben-Hamo-Cybertruck-swastika-Michael-Lewis.html\n",
      "\n",
      "Fetching article [9]: https://www.dailymail.co.uk/sciencetech/article-14506405/Elon-Musk-HALT-Cybertruck-deliveries.html\n",
      "\n",
      "Fetching article [10]: https://www.business-standard.com/world-news/toronto-excludes-tesla-vehicles-from-ev-incentive-due-to-us-trade-war-125031800134_1.html\n",
      "\n",
      "\n",
      "Final Output:\n",
      "\n",
      "{'Company': 'tesla', 'Articles': [{'Title': 'The reason Tesla doesn’t pay taxes', 'Summary': \"The article criticizes Democrats for targeting Tesla's low tax payments while ignoring the company's utilization of tax breaks and government mandates designed to incentivize green energy and R&D. \\n\\nWhile a report by the left-leaning Institute on Taxation and Economic Policy states Tesla paid minimal federal income tax despite significant profits, the article argues this is a result of legitimate tax advantages, not wrongdoing. \\n\\nIt highlights that Tesla's profits are largely derived from selling regulatory credits to other automakers required to comply with EV mandates, suggesting these mandates, not tax breaks, are the primary driver of Tesla's success.  \\n\\nThe article further points out that Tesla's interest income, boosted by Biden's inflation, and unrealized capital gains from bitcoin holdings also contribute to its low tax liability.  Essentially, the article frames the criticism of Tesla as politically motivated, arguing that Democrats are unfairly targeting a company benefiting from policies they themselves supported.\", 'Sentiment': 'Positive', 'Topics': ['Institute on Taxation and Economic Policy', 'EV', 'Tesla', 'Biden']}, {'Title': 'Error extracting content', 'Summary': \"Please provide the article you'd like me to summarize. I need the text content to be able to create a summary. 😊\", 'Sentiment': 'Positive', 'Topics': []}, {'Title': 'Error extracting content', 'Summary': 'Please provide the article content so I can summarize it for you. \\n\\nI need the actual text of the article to be able to understand it and create a summary.', 'Sentiment': 'Positive', 'Topics': []}, {'Title': 'Mark Rober’s Tesla video was more than a little weird', 'Summary': 'This article discusses a viral video by YouTuber Mark Rober comparing Tesla\\'s Autopilot system with a lidar-equipped vehicle in a simulated crash test. \\n\\n**Here are the key points:**\\n\\n* **The Test:** Rober\\'s video showed a lidar-equipped car successfully stopping before hitting a wall while a Tesla Model Y crashed into it. \\n* **Controversy:** The video sparked debate, with some praising Rober for exposing Autopilot\\'s limitations and others accusing him of faking the results or having ulterior motives (potentially due to the prominence of lidar company Luminar).\\n* **Author\\'s Critique:** The author criticizes Rober\\'s video title, \"Can you fool a self-driving car,\" arguing that neither vehicle was truly self-driving. Autopilot requires driver attention and is not a fully autonomous system.\\n* **Unanswered Questions:** The author mentions reaching out to Rober but hasn\\'t received a response regarding the controversies surrounding the video.\\n\\n\\nThe article highlights the ongoing confusion surrounding advanced driver-assistance systems (ADAS) like Autopilot and the importance of accurately representing their capabilities.', 'Sentiment': 'Positive', 'Topics': ['Mark Rober', 'Luminar', 'Critique:*', 'ADAS', 'Rober', 'Autopilot', 'Tesla']}, {'Title': 'Tesla allegedly exploiting Canada’s EV rebate program: All you need to know', 'Summary': \"Tesla is facing scrutiny in Canada for allegedly exploiting the country's EV rebate program before it was discontinued. \\n\\n**Key Points:**\\n\\n* **Surge in Sales:** Tesla experienced an unusual spike in sales in British Columbia during January, coinciding with the suspension of the federal EV rebate program.\\n* **Potential Abuse:** Canadian officials are investigating whether Tesla strategically registered a large number of vehicles just before the rebate program ended, securing millions in government subsidies.\\n* **Government Response:** British Columbia has revoked government subsidies for Tesla chargers, batteries, and inverters as part of broader trade tensions with the United States.\\n* **Industry Outcry:** The Canadian Automobile Dealers Association (CADA) and other industry stakeholders are accusing Tesla of unfair practices, claiming they secured approximately $90 million in subsidies before the program ended.\\n* **Tesla's Performance:** Tesla's stock has significantly declined in 2025, and European sales are also declining.\\n\\nThe article highlights concerns about Tesla's actions and the potential for companies to exploit government incentives. It also showcases the escalating trade tensions between Canada and the United States.\", 'Sentiment': 'Positive', 'Topics': ['the United States', 'EV', 'Tesla', 'The Canadian Automobile Dealers Association', 'Canada', 'British Columbia']}, {'Title': \"Tesla Model 3, Model Y's Homologation Process Initiated In India\", 'Summary': \"Tesla is making significant progress towards launching its electric vehicles in India. \\n\\nThey have formally applied for homologation, a crucial certification process, for the Model Y and Model 3. This application, filed with Tesla India Motor & Energy Pvt. Ltd., ensures the vehicles meet Indian safety and emission standards.\\n\\nThis move comes as Tesla finalizes its first showroom in India and actively hires for various roles. The timeline coincides with ongoing discussions about a free trade agreement between India and the US, which could potentially lower tariffs and benefit Tesla's entry into the market.\\n\\nTesla's Indian launch has been anticipated for over three years, but previous delays stemmed from disagreements over trade terms.\", 'Sentiment': 'Positive', 'Topics': ['Tesla India Motor & Energy Pvt. Ltd.', 'Tesla', 'India', 'US']}, {'Title': 'Error extracting content', 'Summary': 'Please provide the article content so I can summarize it for you.  \\n\\nI need the text of the article to be able to understand it and create a summary.', 'Sentiment': 'Positive', 'Topics': []}, {'Title': 'Error extracting content', 'Summary': 'You provided me with \"No content extracted.\" There is no article for me to summarize. \\n\\nPlease provide the article text so I can help you!', 'Sentiment': 'Neutral', 'Topics': []}], 'Final Sentiment Analysis': 'Sentiment Analysis Summary for tesla News Coverage:\\n    - Positive Articles: 7 (87.5%)\\n    - Negative Articles: 0 (0.0%)\\n    - Neutral Articles: 1 (12.5%)\\n\\n    Insights:\\n    - The majority of news articles are positive.\\n    - There is 87.5% difference between positive and negative coverage.\\n    - This analysis suggests that media perception of tesla is generally favorable.'}\n",
      "Hindi Translation: टेस्ला समाचार कवरेज के लिए भावना विश्लेषण सारांश:\n",
      "    - सकारात्मक लेख: 7 (87.5%)\n",
      "    - नकारात्मक लेख: 0 (0.0%)\n",
      "    - तटस्थ लेख: 1 (12.5%)\n",
      "\n",
      "    अंतर्दृष्टि:\n",
      "    - अधिकांश समाचार लेख सकारात्मक हैं।\n",
      "    - सकारात्मक और नकारात्मक कवरेज के बीच 87.5% अंतर है।\n",
      "    - इस विश्लेषण से पता चलता है कि टेस्ला की मीडिया धारणा आम तौर पर अनुकूल है।\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 950, which is longer than the specified 800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the main points from the text:\n",
      "\n",
      "* **YouTuber Mark Rober conducted a test comparing Tesla's camera-only Autopilot system to a vehicle equipped with lidar.** \n",
      "* **The lidar-equipped vehicle stopped before hitting a fake wall, while the Tesla Model Y crashed into it.**\n",
      "* **Some criticism of Rober's video claims he faked the test, manipulated footage, and received payment from lidar company Luminar (which he denies).**\n",
      "* **Rober released \"raw footage\" showing Autopilot was engaged before the crash, addressing claims it wasn't.**\n",
      "\n",
      "\n",
      "The text also touches on Tesla's alleged involvement in fraud with Canada's EV rebate program, but this is not directly related to Rober's video. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from newspaper import Article\n",
    "import time\n",
    "import os\n",
    "import spacy\n",
    "from dotenv import load_dotenv\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.schema import Document\n",
    "from textblob import TextBlob  \n",
    "from deep_translator import GoogleTranslator\n",
    "from gtts import gTTS\n",
    "from urllib.parse import urljoin\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain, create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "# Load NLP model for topic extraction\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Initialize LLM\n",
    "llm = ChatGroq(groq_api_key=api_key, model=\"Gemma2-9b-It\")\n",
    "\n",
    "def fetch_news_links_bing(company, max_articles=10):\n",
    "    \"\"\"Fetch at least 5 unique news article links from Bing News\"\"\"\n",
    "    search_url = f\"https://www.bing.com/news/search?q={company.replace(' ', '+')}\"\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "    response = requests.get(search_url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    news_links = set()\n",
    "    base_url = \"https://www.bing.com\"\n",
    "\n",
    "    for link in soup.find_all(\"a\", href=True):\n",
    "        url = urljoin(base_url, link[\"href\"])  # Convert to absolute URL\n",
    "        if url.startswith(\"http\") and \"bing.com\" not in url and url not in news_links:\n",
    "            news_links.add(url)\n",
    "        if len(news_links) >= max_articles:\n",
    "            break\n",
    "\n",
    "    if len(news_links) < max_articles:\n",
    "        print(f\"Warning: Only {len(news_links)} unique articles found.\")\n",
    "    \n",
    "    return list(news_links)[:max_articles]\n",
    "\n",
    "def extract_article_text(url):\n",
    "    \"\"\"Extract clean article text using newspaper3k\"\"\"\n",
    "    try:\n",
    "        article = Article(url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "\n",
    "        if not article.text.strip():\n",
    "            return \"Error extracting content\", \"No content extracted.\"\n",
    "\n",
    "        return article.title, article.text\n",
    "    except Exception as e:\n",
    "        return \"Error extracting content\", f\"Error extracting content: {str(e)}\"\n",
    "\n",
    "# Function to process query related to articles\n",
    "def process_query(texts, query):\n",
    "    # Split the text into chunks\n",
    "    text_splitter = CharacterTextSplitter(separator=\"\\n\", chunk_size=800, chunk_overlap=200, length_function=len)\n",
    "    split_texts = text_splitter.split_text(texts)\n",
    "\n",
    "    embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "    db = FAISS.from_texts(split_texts, embeddings)\n",
    "\n",
    "    retriever = db.as_retriever()\n",
    "    model = ChatGroq(model=\"Gemma2-9b-It\", groq_api_key=api_key)\n",
    "\n",
    "    # Contextualization prompt\n",
    "    contextualize_q_system_prompt = (\n",
    "        \"Given a chat history and the latest user question \"\n",
    "        \"which might reference context in the chat history, \"\n",
    "        \"formulate a standalone question which can be understood \"\n",
    "        \"without the chat history. Do NOT answer the question, \"\n",
    "        \"just reformulate it if needed and otherwise return it as is.\"\n",
    "    )\n",
    "    contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", contextualize_q_system_prompt),\n",
    "            MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "            (\"human\", \"{input}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    history_aware_retriever = create_history_aware_retriever(model, retriever, contextualize_q_prompt)\n",
    "\n",
    "    # System prompt for answering questions\n",
    "    system_prompt = (\n",
    "        \"You are an AI assistant that helps summarize and answer questions from documents.\\n\\n\"\n",
    "        \"Context:\\n{context}\\n\\n\"\n",
    "        \"Chat History:\\n{chat_history}\\n\\n\"\n",
    "        \"User Question:\\n{input}\"\n",
    "    )\n",
    "\n",
    "    qa_prompt = ChatPromptTemplate.from_template(system_prompt)\n",
    "\n",
    "    question_answer_chain = create_stuff_documents_chain(model, qa_prompt)\n",
    "    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "\n",
    "    chat_history = []\n",
    "    response = rag_chain.invoke({\"input\": query, \"chat_history\": chat_history})\n",
    "\n",
    "    return response['answer']\n",
    "\n",
    "def extract_key_topics(text):\n",
    "    \"\"\"Extracts key topics using spaCy NLP\"\"\"\n",
    "    doc = nlp(text)\n",
    "    topics = {ent.text for ent in doc.ents if ent.label_ in [\"ORG\", \"GPE\", \"EVENT\", \"PERSON\"]}\n",
    "    return list(topics)\n",
    "\n",
    "def summarize_text(article_text):\n",
    "    \"\"\"Summarizes the article content using LLM\"\"\"\n",
    "    if not article_text.strip():\n",
    "        return \"Summary not available due to extraction failure.\"\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=100)\n",
    "    docs = text_splitter.split_documents([Document(page_content=article_text)])\n",
    "\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=['text'],\n",
    "        template=\"Summarize the following article:\\n{text}\"\n",
    "    )\n",
    "\n",
    "    summary_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "    summary_output = summary_chain.run({\"text\": docs[0].page_content})  # Summarizing first chunk\n",
    "    return summary_output.strip() if summary_output else \"Summary not generated.\"\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    \"\"\"Performs sentiment analysis using TextBlob\"\"\"\n",
    "    sentiment_score = TextBlob(text).sentiment.polarity\n",
    "    return \"Positive\" if sentiment_score > 0 else \"Negative\" if sentiment_score < 0 else \"Neutral\"\n",
    "\n",
    "def generate_sentiment_summary(articles_data):\n",
    "    \"\"\"Generates summary insights based on sentiment analysis\"\"\"\n",
    "    total_articles = len(articles_data)\n",
    "    \n",
    "    if total_articles == 0:\n",
    "        return \"No valid articles were processed for analysis.\"\n",
    "\n",
    "    sentiment_counts = {\"Positive\": 0, \"Negative\": 0, \"Neutral\": 0}\n",
    "\n",
    "    for article in articles_data:\n",
    "        sentiment_counts[article[\"Sentiment\"]] += 1\n",
    "\n",
    "    # Calculate percentages\n",
    "    positive_pct = round((sentiment_counts[\"Positive\"] / total_articles) * 100, 2)\n",
    "    negative_pct = round((sentiment_counts[\"Negative\"] / total_articles) * 100, 2)\n",
    "    neutral_pct = round((sentiment_counts[\"Neutral\"] / total_articles) * 100, 2)\n",
    "\n",
    "    summary = f\"\"\"\n",
    "    Sentiment Analysis Summary for {company_name} News Coverage:\n",
    "    - Positive Articles: {sentiment_counts['Positive']} ({positive_pct}%)\n",
    "    - Negative Articles: {sentiment_counts['Negative']} ({negative_pct}%)\n",
    "    - Neutral Articles: {sentiment_counts['Neutral']} ({neutral_pct}%)\n",
    "\n",
    "    Insights:\n",
    "    - The majority of news articles are {'positive' if positive_pct > negative_pct else 'negative' if negative_pct > positive_pct else 'neutral'}.\n",
    "    - There is {abs(positive_pct - negative_pct)}% difference between positive and negative coverage.\n",
    "    - This analysis suggests that media perception of {company_name} is {('generally favorable' if positive_pct > negative_pct else 'somewhat critical' if negative_pct > positive_pct else 'balanced')}.\n",
    "    \"\"\"\n",
    "    return summary.strip()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    company_name = input(\"Enter company name: \")\n",
    "    news_links = fetch_news_links_bing(company_name, max_articles=10)\n",
    "\n",
    "    articles_data = []\n",
    "    articals_text = []\n",
    "\n",
    "    if news_links:\n",
    "        for idx, link in enumerate(news_links):\n",
    "            print(f\"\\nFetching article [{idx+1}]: {link}\")\n",
    "            time.sleep(1)\n",
    "            \n",
    "            title, article_text = extract_article_text(link)\n",
    "            if article_text.startswith(\"Error\"):\n",
    "                continue  # Skip faulty articles\n",
    "            articals_text.append(article_text)\n",
    "            summary = summarize_text(article_text)\n",
    "            key_topics = extract_key_topics(summary)\n",
    "            sentiment = analyze_sentiment(summary)\n",
    "            \n",
    "            articles_data.append({\n",
    "                \"Title\": title,\n",
    "                \"Summary\": summary,\n",
    "                \"Sentiment\": sentiment,\n",
    "                \"Topics\": key_topics\n",
    "            })\n",
    "\n",
    "    sentiment_summary = generate_sentiment_summary(articles_data)\n",
    "    output_data = {\n",
    "        \"Company\": company_name,\n",
    "        \"Articles\": articles_data,\n",
    "        \"Final Sentiment Analysis\": sentiment_summary\n",
    "    }\n",
    "\n",
    "    print(\"\\n\\nFinal Output:\\n\")\n",
    "    print(output_data)\n",
    "    \n",
    "    english_text = sentiment_summary\n",
    "    hindi_text = GoogleTranslator(source='en', target='hi').translate(english_text)\n",
    "\n",
    "    print(\"Hindi Translation:\", hindi_text)\n",
    "\n",
    "    tts = gTTS(text=hindi_text, lang='hi')  \n",
    "    tts.save(\"hindi_audio.mp3\")  # Save the audio file  \n",
    "\n",
    "    # Step 3: Play the audio file (Windows)\n",
    "    os.system(\"start hindi_audio.mp3\") \n",
    "\n",
    "    query = input(\"Please enter your query related to the article: \")\n",
    "    query_process = process_query(' '.join(articles_data), query)\n",
    "    print(query_process)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
